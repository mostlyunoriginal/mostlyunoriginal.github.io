{
  "hash": "cb8485f8540a75b2b758136f93971c1a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R vs. Python Query Compute Time Example\"\ndescription: \"Here we compare runtime to query a large csv in R and Python\"\ndate: 2025-03-19\nauthor:\n    - name: Lance Couzens\n      url: https://mostlyunoriginal.github.io  \ncategories: [R, Python]\ncitation: \n  url: https://mostlyunoriginal.github.io/posts/2025-03-19-TidyR-to-PolarsPython/\nimage: PReview.webp\ndraft: false\nparams:\n  rows: 1000000\n  cols: 1000\n---\n\n\n\nLet's compare the compute time needed for an equivalent operation between `Python` and `R`. The operation is to:\n\n1.  ingest a largeish csv file (\\~18GB) with 1,000,000 records and 1,000 columns of random normal variates plus one `ID` column,\n2.  group by `ID` (100 records per `ID`),\n3.  summarize as mean for each double/float column,\n4.  filter to `ID`s with any one or more double/float column with a `mean > 0.4`, and\n5.  report how many such rows were found.\n\nIn Python, we will use `polars` with lazy evaluation. In `R`, we will use `dplyr`, `dtplyr`, and `tidytable`. The latter two packages interpret `dplyr` syntax and deploy the `data.table` equivalent for efficiency.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n# Python with polars\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\nstart=datetime.now()\n\nq=(\n    pl.scan_csv(\"big.csv\")\n    .group_by(\"id\")\n    .agg(cs.float().mean())\n    .filter(pl.any_horizontal(cs.float()>.4))\n)\n\ntable=q.collect()\n\nelapsed=datetime.now()-start\n\nprint(f\"{table.height} rows returned\\nelapsed time for query: {elapsed}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n301 rows returned\nelapsed time for query: 0:00:31.164063\n```\n\n\n:::\n:::\n\n\n\n# R\n\nNote that `data.table::fread()` is used for all R examples, as we're really just focusing on the data manipulation approach penalties.\n\n## Plain dplyr\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(hms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'hms'\n\nThe following object is masked from 'package:lubridate':\n\n    hms\n```\n\n\n:::\n\n```{.r .cell-code}\nstart<-Sys.time()\n\nrows<-data.table::fread(\"big.csv\") %>%\n  group_by(id) %>%\n  summarize(across(where(is.double),mean),.groups=\"keep\") %>%\n  filter(if_any(where(is.double),~.x>.4)) %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n301 rows returned\nelapsed time for query: 00:01:57.947875\n```\n\n\n:::\n:::\n\n\n\n## dtplyr\n\nThis is stylistically the `R` version that is most similar to the `polars` approach, but it does come with some downsides in that not all `dplyr` functionality is supported. In this example that is most obvious in the inability to use tidyselect helpers in `summarize()` and `filter()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dtplyr,warn.conflicts=F)\n\nstart<-Sys.time()\n\nbig<-data.table::fread(\"big.csv\")\n\nvarnames<-setdiff(colnames(big),\"id\")\n\nrows<-lazy_dt(big) %>%\n  group_by(id) %>%\n  summarize(across(all_of(varnames),mean),.groups=\"keep\") %>%\n  filter(if_any(all_of(varnames),~.x>.4)) %>%\n  collect() %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n301 rows returned\nelapsed time for query: 00:00:43.045766\n```\n\n\n:::\n:::\n\n\n\n## tidytable\n\nThis should be computationally comparable to the `dtplyr` approach as both are deploying `data.table` behind the scenes, but this approach has the benefit of preserving the plain `dplyr` syntax, including the ability to use tidyselect helpers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart<-Sys.time()\n\nrows<-data.table::fread(\"big.csv\") %>%\n  tidytable::group_by(id) %>%\n  tidytable::summarize(tidytable::across(where(is.double),mean),.groups=\"keep\") %>%\n  tidytable::filter(tidytable::if_any(where(is.double),~.x>.4)) %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n301 rows returned\nelapsed time for query: 00:00:56.428901\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n# Conclusions\n\nAfter rendering this several time on both my work PC (2022 Windows laptop with 2.5GHz i7 and 32GB RAM) and my Mac at home (2023 M2 Max with 32GB memory) the general takeaway is that the plain vanilla `dplyr` approach is anywhere from 3 to 8 times slower than `polars`. On the Mac, `dtplyr` and `polars` are very close with `tidytable` coming in only a tad slower. On the Windows machine, both `dtplyr` and `tidytable` are about 2-3x `polars`.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}