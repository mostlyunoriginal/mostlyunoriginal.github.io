{
  "hash": "1ae3f942657d986726c3b74d4ba09132",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R vs. Python Query Compute Time Example\"\ndescription: \"Here we compare runtime to query a large csv in R and Python\"\ndate: 2025-03-19\nauthor:\n    - name: Lance Couzens\n      url: https://mostlyunoriginal.github.io  \ncategories: [R, Python]\ncitation: \n  url: https://mostlyunoriginal.github.io/posts/2025-03-19-TidyR-to-PolarsPython/\nimage: PReview.webp\ndraft: false\nparams:\n  rows: 1000\n  cols: 10\n---\n\n\n\nLet's compare the compute time needed for an equivalent operation between `Python` and `R`. The operation is to:\n\n1.  ingest a largeish csv file with 1,000 records and 10 columns of random normal variates plus one `ID` column,\n2.  group by `ID` (100 records per `ID`),\n3.  summarize as mean for each double/float column,\n4.  filter to `ID`s with any one or more double/float column with a `mean > 0.4`, and\n5.  report how many such rows were found.\n\nIn Python, we will use `polars` with lazy evaluation. In `R`, we will use `dplyr`, `dtplyr`, and `tidytable`. The latter two packages interpret `dplyr` syntax and deploy the `data.table` equivalent for efficiency.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n# Python with polars\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\nstart=datetime.now()\n\nq=(\n    pl.scan_csv(\"big.csv\")\n    .group_by(\"id\")\n    .agg(cs.float().mean())\n    .filter(pl.any_horizontal(cs.float()>.4))\n)\n\ntable=q.collect()\n\nelapsed=datetime.now()-start\n\nprint(f\"{table.height} rows returned\\nelapsed time for query: {elapsed}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n991 rows returned\nelapsed time for query: 0:00:00.061715\n```\n\n\n:::\n:::\n\n\n\n# R\n\nNote that `data.table::fread()` is used for all R examples, as we're really just focusing on the data manipulation approach penalties.\n\n## Plain dplyr\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(hms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'hms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:lubridate':\n\n    hms\n```\n\n\n:::\n\n```{.r .cell-code}\nstart<-Sys.time()\n\nrows<-data.table::fread(\"big.csv\") %>%\n  group_by(id) %>%\n  summarize(across(where(is.double),mean),.groups=\"keep\") %>%\n  filter(if_any(where(is.double),~.x>.4)) %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n991 rows returned\nelapsed time for query: 00:00:00.249162\n```\n\n\n:::\n:::\n\n\n\n## dtplyr\n\nThis is stylistically the `R` version that is most similar to the `polars` approach, but it does come with some downsides in that not all `dplyr` functionality is supported. In this example that is most obvious in the inability to use tidyselect helpers in `summarize()` and `filter()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dtplyr,warn.conflicts=F)\n\nstart<-Sys.time()\n\nbig<-data.table::fread(\"big.csv\")\n\nvarnames<-setdiff(colnames(big),\"id\")\n\nrows<-lazy_dt(big) %>%\n  group_by(id) %>%\n  summarize(across(all_of(varnames),mean),.groups=\"keep\") %>%\n  filter(if_any(all_of(varnames),~.x>.4)) %>%\n  collect() %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n991 rows returned\nelapsed time for query: 00:00:00.045482\n```\n\n\n:::\n:::\n\n\n\n## tidytable\n\nThis should be computationally comparable to the `dtplyr` approach as both are deploying `data.table` behind the scenes, but this approach has the benefit of preserving the plain `dplyr` syntax, including the ability to use tidyselect helpers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart<-Sys.time()\n\nrows<-data.table::fread(\"big.csv\") %>%\n  tidytable::group_by(id) %>%\n  tidytable::summarize(tidytable::across(where(is.double),mean),.groups=\"keep\") %>%\n  tidytable::filter(tidytable::if_any(where(is.double),~.x>.4)) %>%\n  nrow()\n\nend<-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n991 rows returned\nelapsed time for query: 00:00:00.054398\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n##",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}