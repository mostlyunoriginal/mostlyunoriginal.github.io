{
  "hash": "a56e19ddd98021661a966352afaf029f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Building a Technical Trading Data & Analytics Pipeline\"\ndescription: \"This post is the first in a series chronicling a personal project: setting up a technical investment data and analytics pipeline with Python and R\"\ndate: 2025-04-01\nauthor:\n    - name: Lance Couzens\n      url: https://mostlyunoriginal.github.io  \ncategories: [R, Python, Investing]\ncitation: \n  url: https://mostlyunoriginal.github.io/posts/2025-04-01-Tech-Invest-Pipeline-Part1/\nimage: PReview.webp\ndraft: false\nparams:\n    ref_date: '2025-04-01'\n    ticker: 'AAPL'\n    window: 120\n    sma_l: 50\n    sma_s: 20\n---\n\n\n\n## Background\n\nAs a person with a strong background in analytics and a love of programming, I've always wanted to have a go at technical investing, using my own pipeline. I've finally taken the project on in earnest, and I'm going to chronicle the twists and turns it takes on the blog---this is Part 1.\n\nSo, what do I mean by 'pipeline' in this context? Essentially, I mean ingesting market data, algorithmically curating buy candidates, tracking existing positions for sell signals, and all the nitty gritty in-betweens that entails. I envision four high-level components:\n\n1.  Data ingestion and transformation,\n\n2.  Application of a model to identify and rank buy candidates,\n\n3.  Daily, automated report creation to help me make decisions on buy candidates, and\n\n4.  Daily/intraday reporting/monitoring for existing positions.\n\nI've started on numbers 1 and 3, so I will cover some of that here.\n\n## Data Ingestion\n\nI'm going to focus exclusively on stocks to start, and I'll be getting my data from Polygon.io---they have a variety of data offerings across various personal and business tiers (including a free option). I'll be using the Stocks Starter plan, which provides a decent amount of historical data aggregated in flat files by day or minute via Amazon S3 as well as near-real-time data via API.\n\nMy core data object that will serve as input to the curation model will be a Python (Polars) DataFrame of daily aggregates for all U.S. stocks (\\~10K) over a flexible window of time through the prior trading day. I'll build the DataFrame from flat files using the Python Boto3 SDK for S3 and two custom functions.\n\n### Function 1: List Files\n\nThis function returns a list of file names satisfying parameterized criteria (day vs. minute, last day, window size, etc.).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef list_hist_files(\n        kind='day_aggs',\n        last_day='2025-03-28',\n        window=30,\n        prefix='us_stocks_sip',\n        bucket_name='flatfiles',\n        bookend=False,\n    ):\n\n    session=boto3.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )\n\n    s3=session.client(\n        's3',\n        endpoint_url='https://files.polygon.io',\n        config=Config(signature_version='s3v4'),\n    )\n\n    paginator=s3.get_paginator('list_objects_v2')\n\n    dates=[]\n    end_date=datetime.strptime(last_day,'%Y-%m-%d')\n    for delta in range(window+1):\n        temp_past_date=end_date-timedelta(days=delta)\n        dates.append(datetime.strftime(temp_past_date,'%Y-%m-%d'))\n\n    files=[]\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n        for obj in page['Contents']:\n            if obj['Key'].find(kind)>=0 and re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',obj['Key']) in dates: \n                files.append(obj['Key'])\n\n    if bookend and len(files)>2:\n        files=[files[0],files[-1]]\n\n    return files\n```\n:::\n\n\n\n### Function 2: Ingest Files\n\nThe second function reads a single, dated file for the full market or for an optional subset of tickers into memory and returns a Polars DataFrame. This function has a simple positional parameterization, as it's intended to be called via the `itertools.starmap()` functional.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_hist_data(file,tickers):\n\n    date=re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',file)\n\n    session=boto3.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )\n\n    s3=session.client(\n        's3',\n        endpoint_url='https://files.polygon.io',\n        config=Config(signature_version='s3v4'),\n    )\n\n    response=s3.get_object(Bucket='flatfiles',Key=file)\n    compressed_data=response[\"Body\"].read()\n\n    with gzip.GzipFile(fileobj=io.BytesIO(compressed_data),mode=\"rb\") as f:\n        if tickers: df=pl.scan_csv(f).filter(pl.col('ticker').is_in(tickers)).collect()\n        else: df=pl.read_csv(f)\n\n    return df.insert_column(1,pl.lit(date).alias(\"date\"))\n```\n:::\n\n\n\n## Data Transormation\n\nNext, I pull in the data and create some metrics, including candlesticks, short and long simple moving averages, moving average convergence/divergence (MACD) indicator and signal (its moving average), and the relative strength index (RSI). Note that I convert to a Pandas DataFrame here to make the data readable in a subsequent R blockâ€”that's just a byproduct of this post being written in Quarto. This portion of the pipeline will be scripted when implemented.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport itertools as it\nimport boto3\nfrom botocore.config import Config\nimport tempfile\nimport gzip\nimport io\nimport re\nfrom datetime import datetime, timedelta\nfrom polygon import RESTClient\nimport polars.selectors as cs\nfrom dataclasses import asdict\n\nwith open(\"/Users/lance/Desktop/TechInvest/scripts/sandbox/01_GetHistAggs.py\") as script:\n    exec(script.read())\n\nwith open(\"/Users/lance/Desktop/TechInvest/keys.py\") as script:\n    exec(script.read())\n\niterator=it.product(\n    list_hist_files(kind=\"day_aggs\",last_day=r.params[\"ref_date\"],window=r.params[\"window\"],bookend=False),\n    [[r.params[\"ticker\"]]],\n)\n\ndf=it.starmap(get_hist_data,iterator)\n\ndf=(\n    pl.concat(list(df))\n    .lazy()\n    .sort(\"ticker\",\"window_start\")\n    .with_columns(\n        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_l\"]).over(\"ticker\").alias(\"sma_l\"),\n        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_s\"]).over(\"ticker\").alias(\"sma_s\"),\n        (pl.col(\"close\").ewm_mean(span=12,min_samples=12)-\n            pl.col(\"close\").ewm_mean(span=26,min_samples=26)\n        ).over(\"ticker\").alias(\"MACD\"),\n        (pl.col(\"close\")*2-pl.col(\"close\").rolling_sum(window_size=2)).over(\"ticker\").alias(\"rsi_diff\"),\n        pl.when(pl.col(\"close\")>pl.col(\"open\")).then(1)\n        .otherwise(-1)\n        .alias(\"candle_color\"),\n        pl.max_horizontal(\"open\",\"close\").alias(\"candle_high\"),\n        pl.min_horizontal(\"open\",\"close\").alias(\"candle_low\"),\n        pl.mean_horizontal(\"open\",\"close\").alias(\"candle_mid\"),\n    )\n    .with_columns(\n        pl.col(\"MACD\").ewm_mean(span=9,min_samples=9).over(\"ticker\").alias(\"signal\"),\n        pl.when(pl.col(\"rsi_diff\")>0).then(\"rsi_diff\")\n        .otherwise(0)\n        .alias(\"U\"),\n        pl.when(pl.col(\"rsi_diff\")<0).then(-pl.col(\"rsi_diff\"))\n        .otherwise(0)\n        .alias(\"D\"),\n    )\n    .with_columns(\n        (pl.col(\"MACD\")-pl.col(\"signal\")).alias(\"histogram\"),\n        ((pl.col(\"U\").ewm_mean(min_samples=14,alpha=1/14))/(pl.col(\"D\").ewm_mean(min_samples=14,alpha=1/14))).alias(\"RS\"),\n    )\n    .with_columns((100-100/(1+pl.col(\"RS\"))).alias(\"RSI\"))\n    .filter(pl.col(\"signal\").is_not_null())\n    .collect()\n    .to_pandas()\n)\n```\n:::\n\n\n\n## A Graphics Template for Buy Candidate Analysis and Position Monitoring\n\nI could probably develop these graphics in Python, but I've just got way too much `ggplot` experience at this point and would rather do this part in R. The idea is to present a consistent set of metrics I can use to make buy/sell decisions. I *think* these will be embedded in ticker-specific html reports along with other relevant information as of yet undetermined. Here's the code with some example output for Apple stock over a 10-week window ending on April 1, 2025.\n\n\n\n::: {.cell fig.fullwidth='true'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(patchwork)\n\ntheme<-theme_set(theme_linedraw())+\n    theme_update(\n        axis.text.x=element_blank(),axis.ticks.x=element_blank(),\n    )\n\ng2<-ggplot(py$df,aes(x=as_date(date)))+\n    geom_hline(yintercept=0,linewidth=.5,color=\"lightgrey\")+\n    geom_col(aes(y=histogram,fill=MACD>signal))+\n    scale_x_date(\n        limits=c(min(as_date(py$df$date))-1,max(as_date(py$df$date))+1),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=1),\n    )+\n    scale_y_continuous()+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")\n\nu<-layer_scales(g2)$x$limits[[2]] %>% as_date()\nl<-layer_scales(g2)$x$limits[[1]] %>% as_date()\n\ng1<-ggplot(py$df,aes(x=as_date(date)))+\n    geom_hline(yintercept=0,linetype=2,linewidth=.5)+\n    geom_line(aes(y=signal),color=\"#92C5DE\",linewidth=1.1)+\n    geom_line(aes(y=MACD),color=\"#F4A582\",linewidth=1.1)+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=1),\n    )+\n    scale_y_continuous()+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")\n\ng3<-ggplot(py$df,aes(x=as_date(date)))+\n    theme_update(\n        axis.text.x=element_text(angle=45,hjust=1,vjust=1),\n        axis.ticks.x=element_line(),\n    )+\n    annotate(geom=\"rect\",fill=\"#000000\",xmin=l,xmax=u,ymin=30,ymax=70,alpha=0.1)+\n    geom_line(aes(y=RSI),linewidth=1.1,color=\"#F4A582\")+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=1),\n    )+\n    scale_y_continuous(limits=c(0,100),breaks=c(0,30,70,100))+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")+\n    theme_update(\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n    )\n\np<-ggplot(py$df,aes(x=as_date(date)))+\n    geom_line(aes(y=sma_l),color=\"#92C5DE\",linewidth=1.1)+\n    geom_line(aes(y=sma_s),color=\"#F4A582\",linewidth=1.1)+\n    geom_linerange(aes(ymax=high,ymin=low,color=factor(candle_color)))+\n    geom_tile(\n        aes(\n            y=candle_mid,\n            height=candle_high-candle_low,\n            width=.8,\n            fill=factor(candle_color),\n        ),\n    )+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=1),\n    )+\n    scale_y_continuous(labels=scales::dollar)+\n    labs(y=NULL,x=NULL)+\n    ggtitle(str_glue(\"Ticker: {params$ticker}\"))+\n    guides(color=\"none\",fill=\"none\")\n\np / g1 / g2 / g3 + plot_layout(nrow=4,heights=c(3,1,1,1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=960}\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}