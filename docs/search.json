[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Code Blog",
    "section": "",
    "text": "New Features in cendat ver 0.4.1\n\n\n\nPython\n\ncendat\n\nCensus\n\nAPI\n\n\n\nThis post covers some new cendat features.\n\n\n\n\n\nAug 22, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API\n\n\n\nPython\n\ncendat\n\nCensus\n\nAPI\n\n\n\nThis post introduces cendat and demonstrates its use through examples.\n\n\n\n\n\nAug 13, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nThread Pooling a Python Process to Build a Stacked ACS DataFrame\n\n\n\nPython\n\nThread Pooling\n\n\n\nThis post demonstrates how to take a standard list comprehension of calls to the Census API and pool it for dramatic time savings.\n\n\n\n\n\nAug 5, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Technical Trading Data & Analytics Pipeline\n\n\n\nR\n\nPython\n\nInvesting\n\n\n\nThis post is the first in a series chronicling a personal project: setting up a technical investment data and analytics pipeline with Python and R\n\n\n\n\n\nApr 1, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Working with a List of ggplot Objects in R\n\n\n\nR\n\nGraphics\n\nLists\n\nCensus API\n\n\n\nHere we make a list of choropleth maps\n\n\n\n\n\nMar 22, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nDecoupling Dynamic Code from a Static R Codebase\n\n\n\nR\n\nQuality\n\n\n\nHere we look at 2 ways to isolate the code that may change from the code that doesn’t need to\n\n\n\n\n\nMar 20, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nR vs. Python Query Compute Time Example\n\n\n\nR\n\nPython\n\n\n\nHere we compare runtime to query a large csv in R and Python\n\n\n\n\n\nMar 19, 2025\n\n\nLance Couzens\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating Some Tidy-Style R Operations with Python and Polars\n\n\n\nR\n\nPython\n\n\n\nThis post lays out a series of examples that helped me break into Python as a Tidy-style R programmer\n\n\n\n\n\nMar 19, 2025\n\n\nLance Couzens\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lance Couzens",
    "section": "",
    "text": "Hello, and welcome to my code-stuff website. I’m a statistician/data professional who is very interested in totally obsessed with open-source programming. Most of the action is happening under the Code Blog. Thanks for stopping by, and don’t hesitate to hit me up via email if you’d like to chat about coding, statistics, photography, or… whatever."
  },
  {
    "objectID": "posts/2025-03-22-Choropleths-and-LIst-Retrieval-Fun/index.html",
    "href": "posts/2025-03-22-Choropleths-and-LIst-Retrieval-Fun/index.html",
    "title": "Building and Working with a List of ggplot Objects in R",
    "section": "",
    "text": "Lists are, in many ways, R’s most powerful objects. They are vectors without type, general and flexible. You can fill them with anything—including other lists—and while this enables some truly useful complexity in our R-based processes, it can also make creating and working with lists daunting, especially for newer R programmers.\nWith this post I make no attempt to explain (let alone fully explain) lists in R. Instead I just hope to showcase one example of doing something fun (and maybe kinda useful-ish?) with them: storing and retrieving income disparity choropleth maps made with the Census API and ggplot2. Everyone loves maps, right?!\nHere’s what we’ll do:\n\nretrieve a state-level data frame with 2023 ACS 5-year poverty estimates via the Census API\ncreate the recode variable prop_below that represents the proportion of the state population with household income below the poverty limit\nsort in descending order by prop_below\nusing the resulting data frame as a parameter file, iterate a custom function that\n\nretrieves county-level median income estimates and polygons for a given state via the Census API\nranks the counties by their median income\ngenerates clean labels for the top and bottom counties\ncreates a plot object containing the county choropleth map for the state\nreturns a list containing the plot, the state abbreviation, and the state name\n\n\nAt this point, we will have a list with 50 elements—one for each state—each of which is a list containing the state-level plots plus state names and abbreviations. We will explore three ways to extract plots from this list.\nFirst, we’ll load the necessary libraries and create our state-level parameter file.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(scales)\n\nyear&lt;-2023\n\npoverty_data&lt;-get_acs(\n  geography=\"state\"\n  ,variables=c(\n      below_p5pov=\"C17002_002\"\n      ,p5_1pov=\"C17002_003\"\n      ,total_population=\"C17002_001\"\n  )\n  ,year=year\n  ,survey=\"acs5\"\n) %&gt;%\n  pivot_wider(\n    id_cols=c(GEOID,NAME)\n    ,names_from=\"variable\"\n    ,values_from=\"estimate\"\n  ) %&gt;%\n  filter(!GEOID %in% c(72,11)) %&gt;%\n  mutate(\n    prop_below=(below_p5pov+p5_1pov)/total_population\n    ,year=.env$year\n  ) %&gt;%\n  arrange(desc(prop_below))\n\nLet’s look at the parameter file to see what our functional process has to work with.\n\npoverty_data\n\n# A tibble: 50 × 7\n   GEOID NAME           total_population below_p5pov p5_1pov prop_below  year\n   &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 28    Mississippi             2851847      244374  299323      0.191  2023\n 2 22    Louisiana               4494539      390425  458344      0.189  2023\n 3 35    New Mexico              2073857      174355  201026      0.181  2023\n 4 54    West Virginia           1728580      131400  156260      0.166  2023\n 5 21    Kentucky                4382816      325901  381579      0.161  2023\n 6 05    Arkansas                2944742      209434  262349      0.160  2023\n 7 01    Alabama                 4913932      352821  415364      0.156  2023\n 8 40    Oklahoma                3872738      276048  317772      0.153  2023\n 9 45    South Carolina          5072217      340381  379339      0.142  2023\n10 48    Texas                  29016925     1845809 2159608      0.138  2023\n# ℹ 40 more rows\n\n\nAnd here we fill our list: maplist. To do that, we iterate over our parameter file with the pmap() functional and an anonymous function containing the guts of our process.\n\nmaplist&lt;-poverty_data %&gt;%\n  pmap( #use pmap so we can provide df as parameter file\n    \n    function(...){ #function takes in all variables in df because of dots param\n      \n      parms&lt;-rlang::list2(...) #extract all var values for current iteration into named list\n      \n      #save plot to p\n      p&lt;-get_acs( #api call returns county-level data with polygons\n        geography=\"county\"\n        ,variables=\"B19013_001\" #median income\n        ,state=parms$GEOID #note use of parms list\n        ,geometry=TRUE #include polygons\n        ,year=parms$year #again here\n      ) %&gt;%\n        mutate(\n          goodlabel=case_when(\n            rank(estimate,na.last=NA,ties.method=\"first\")==1|\n              percent_rank(estimate)==1\n              ~str_replace(NAME,\"(.+)(,.+)\",\"\\\\1\") %&gt;% str_remove(\" County\")\n            ,TRUE~NA_character_\n          )\n        ) %&gt;%\n        ggplot()+\n          #geom for plotting shapefile polygons\n          geom_sf(\n              size=0.05\n              ,color=\"#000000\"\n              ,aes(fill=as.numeric(estimate))\n          )+\n          geom_sf_label(\n              aes(label=goodlabel)\n              ,color=\"#000000\"\n              ,vjust=1\n          )+\n          coord_sf(crs=4326)+\n          scale_fill_viridis_c(\n            option=\"viridis\"\n            ,breaks=seq(0,200000,by=10000)\n            ,labels=dollar\n          )+\n          labs(\n            title=str_glue(\"{parms$NAME} Median Income by County\"),\n            subtitle=str_glue(\n              \"American Community Survey 5-Year Estimates {parms$year-4}-{parms$year}\\n\"\n              ,\"Highest and Lowest Income Counties Labelled\"\n            )\n          )+\n          guides(fill=guide_colorbar(\"Median\\nPast-Year\\nHH Income\"))+\n          theme_bw()+\n          theme_update(legend.key.height=unit(.35,\"in\"))\n      \n      list(\"state_abb\"=parms$state_abb,\"state\"=parms$NAME,\"plot\"=p)\n      \n    }\n    \n  )\n\nAt this point, maplist has been populated, and we can extract plot objects from it. First, let’s try just returning the first element. Recall that because our parameter file was sorted highest to lowest in terms of the proportion of the state population with household income below the poverty limit, the first element of our list will contain a plot for the most impoverished state.\n\npluck(maplist,1)$plot\n\n\n\n\n\n\n\n\nWe can also walk over the list to present ranges of its elements. Here we look at the 5 least impoverished states. Note that in this case we need an explicit print() to force the plots out of the walk functional environment.\n\nwalk(50:46,~pluck(maplist,.x)$plot %&gt;% print())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr we can extract the map corresponding to a specific state of interest. We can do this because we loaded each element of maplist with a list containing both a plot object and state identifiers.\n\ndetect(maplist,~.x$state==\"New York\")$plot\n\n\n\n\n\n\n\ndetect(maplist,~.x$state==\"Texas\")$plot\n\n\n\n\n\n\n\ndetect(maplist,~.x$state==\"California\")$plot\n\n\n\n\n\n\n\n\nIn conclusion… maps are fun, and lists are useful!\n\n\n\nCitationBibTeX citation:@online{couzens2025,\n  author = {Couzens, Lance},\n  title = {Building and {Working} with a {List} of Ggplot {Objects} in\n    {R}},\n  date = {2025-03-22},\n  url = {https://mostlyunoriginal.github.io/posts/2025-03-22-Choropleths-and-LIst-Retrieval-Fun/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCouzens, Lance. 2025. “Building and Working with a List of Ggplot\nObjects in R.” March 22, 2025. https://mostlyunoriginal.github.io/posts/2025-03-22-Choropleths-and-LIst-Retrieval-Fun/."
  },
  {
    "objectID": "posts/2025-08-05-Python-Thread-Pooling/index.html",
    "href": "posts/2025-08-05-Python-Thread-Pooling/index.html",
    "title": "Thread Pooling a Python Process to Build a Stacked ACS DataFrame",
    "section": "",
    "text": "As part of a machine learning class I’ve been building for a client, I’ve provided the option to join county-level ACS estimates onto the training and prediction data. County data require state-level calls to the Census API, and including all states and a few years, sequential pulls were proving to be a significant bottleneck. Swapping the sequential process out in favor of a pooled one yielded dramatic time savings. This post demonstrates that change via an abstracted, simplified example."
  },
  {
    "objectID": "posts/2025-08-05-Python-Thread-Pooling/index.html#background",
    "href": "posts/2025-08-05-Python-Thread-Pooling/index.html#background",
    "title": "Thread Pooling a Python Process to Build a Stacked ACS DataFrame",
    "section": "",
    "text": "As part of a machine learning class I’ve been building for a client, I’ve provided the option to join county-level ACS estimates onto the training and prediction data. County data require state-level calls to the Census API, and including all states and a few years, sequential pulls were proving to be a significant bottleneck. Swapping the sequential process out in favor of a pooled one yielded dramatic time savings. This post demonstrates that change via an abstracted, simplified example."
  },
  {
    "objectID": "posts/2025-08-05-Python-Thread-Pooling/index.html#data-ingestion",
    "href": "posts/2025-08-05-Python-Thread-Pooling/index.html#data-ingestion",
    "title": "Thread Pooling a Python Process to Build a Stacked ACS DataFrame",
    "section": "Data Ingestion",
    "text": "Data Ingestion\nThe get_acs function pulls county-level 5-year ACS estimates from the Census API for a single state and year. We also do all the importing and environment variable creation we’ll need for both versions of subsequent steps (we want those to be as close to apples-to-apples as possible since we’ll be comparing timing).\n\nimport requests\nimport polars as pl\nimport os\nimport itertools as it\nimport us\nfrom dotenv import load_dotenv\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nload_dotenv()\n\n# states and years to pull ACS 5-year county-level estimates for\nstates = [state.fips for state in us.states.STATES]\nyears = [2020, 2021]\n\n# list of tuples representing cartesian product of individual state and year lists\nstate_year_pairs = list(it.product(states, years))\n\ndef get_acs(\n    state_fips: str, \n    year: int,\n    api_key: str = None, \n    acs_vars: dict = {\n            \"B01001_001E\": \"pop_total\",\n            \"B01002_001E\": \"median_age\",\n            \"B19013_001E\": \"med_hh_income\",\n            \"B23025_005E\": \"unemployed\",\n            \"B25077_001E\": \"med_home_value\",\n            \"B25064_001E\": \"med_rent\",\n        }\n    ):\n\n    url = (\n        f\"https://api.census.gov/data/{year}/acs/\"\n        f\"acs5?get={','.join(acs_vars.keys())}\"\n        f\"&for=county:*&in=state:{state_fips}\"\n        f\"&key={api_key}\"\n    )\n\n    # issue request and parse resulting json\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        data = response.json()\n        if len(data)&gt;1:\n            acs_df = pl.DataFrame(\n                data[1:], \n                schema=data[0],\n                orient=\"row\",\n            ).rename(acs_vars).with_columns(year=pl.lit(year))\n            return acs_df\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n\n    return None"
  },
  {
    "objectID": "posts/2025-08-05-Python-Thread-Pooling/index.html#sequential-implementation-via-list-comprehension",
    "href": "posts/2025-08-05-Python-Thread-Pooling/index.html#sequential-implementation-via-list-comprehension",
    "title": "Thread Pooling a Python Process to Build a Stacked ACS DataFrame",
    "section": "Sequential Implementation via List Comprehension",
    "text": "Sequential Implementation via List Comprehension\nHere we grab and stack the data for all states and two data years via list comprehension.\n\n%%time\n\nall_acs = [\n    get_acs(state_fips, year, api_key=os.getenv(\"CENSUS_API_KEY\"))\n    for state_fips, year in state_year_pairs\n]\n\n# concatenate all results\nif all_acs:\n    stacked_acs = pl.concat(all_acs)\n    print(f\"Data stacked and {stacked_acs.height} records returned\")\nelse:\n    print(\"No data was fetched.\")\n\nData stacked and 6284 records returned\nCPU times: total: 46.9 ms\nWall time: 1min 12s"
  },
  {
    "objectID": "posts/2025-08-05-Python-Thread-Pooling/index.html#a-threaded-alternative",
    "href": "posts/2025-08-05-Python-Thread-Pooling/index.html#a-threaded-alternative",
    "title": "Thread Pooling a Python Process to Build a Stacked ACS DataFrame",
    "section": "A Threaded Alternative",
    "text": "A Threaded Alternative\nHere we leverage the concurrent.futures module from the standard Python library to issue all API calls simultaneously, gathering results asynchronously as they become available.\n\n%%time\n\nall_acs = []\nwith ThreadPoolExecutor(max_workers=len(state_year_pairs)) as executor:\n    futures = {\n        executor.submit(\n          get_acs, \n          state_fips, \n          year, \n          api_key=os.getenv(\"CENSUS_API_KEY\")\n        ): (state_fips, year)\n        for state_fips, year in state_year_pairs\n    }\n    for future in as_completed(futures.keys()):\n        pair = futures[future]\n        try:\n          result = future.result()\n          if result is not None:\n              all_acs.append(result)\n        except Exception as e:\n          print(f\"ERROR: Something went wrong for state={pair[0]}, year={pair[1]}: {e}\")\n\n# concatenate all results\nif all_acs:\n    stacked_acs = pl.concat(all_acs)\n    print(f\"Data stacked and {stacked_acs.height} records returned\")\nelse:\n    print(\"No data was fetched.\")            \n\nData stacked and 6284 records returned\nCPU times: total: 46.9 ms\nWall time: 2.6 s\n\n\nIn this alternative, ThreadPoolExecutor is used as a context manager to issue get_acs calls and “future states” are gathered via a dictionary comprehension. Note, however, that this dictionary flips the script a bit with the returned data frames in the key position and the state, year tuples they correspond to in the values position. This is possible because the returned data frames are unique / hashable, and it allows us to report out for which state and year a failure occurs.\nAs results are returned, they are assessed and appended to the all_acs list for subsequent concatenation. As shown by the timing stats under each code chunk, we see a significant speed up from over 1 minute to just a handful of seconds. Note also that we specify a max_workers count equivalent to the number of times we will be hitting the API. For thread pooling, we’re not limited to our machine’s core count, as we’re not running a truly parallelized process that utilizes multiple cores simultaneously."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html",
    "href": "posts/2025-08-13-Introducing-cendat/index.html",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "",
    "text": "The U.S. Census Bureau makes available through its free, public API mountains of data, though navigating it and acquiring the data you need can be tricky, especially for complex, nested geographic summary levels like those available in American Community Survey (ACS) data products. The Census Bureau provides buckets of features and nuance in its data portal https://data.census.gov, but this doesn’t address the need to acquire data programmatically or in aggregate across the many ‘drill-down’ geographies required for certain summary levels. The API addresses the first problem, but on its own it does nothing to address the second. To acquire estimates at certain levels–even for a single state–may require several thousand API queries, and while data can be downloaded in bulk from https://www2.census.gov, you may have to sift through a lot of what you don’t need to get at what you do. It’s also not well-suited for obtaining data in-script.\ncendat aims to address the programmatic need while leveraging automatic query building and concurrency to quickly and easily explore the API’s data offerings, and pull in the data you need. There are other Census API wrapper libraries available for Python, and I honestly haven’t assessed their capabilities more than superficially, but I like building my own tools, so here we are!\ncendat can be installed with pip\npip install cendat\nand you can also install pandas and/or polars at the same time to enable optional methods to work with acquired data\npip install cendat[pandas]\npip install cendat[polars]\npip install cendat[all]\nYou can check out its documentation at https://pypi.org/project/cendat/.\nYou need an API key to get the most out of cendat–you can get one here: https://api.census.gov/data/key_signup.html."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#background",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#background",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "",
    "text": "The U.S. Census Bureau makes available through its free, public API mountains of data, though navigating it and acquiring the data you need can be tricky, especially for complex, nested geographic summary levels like those available in American Community Survey (ACS) data products. The Census Bureau provides buckets of features and nuance in its data portal https://data.census.gov, but this doesn’t address the need to acquire data programmatically or in aggregate across the many ‘drill-down’ geographies required for certain summary levels. The API addresses the first problem, but on its own it does nothing to address the second. To acquire estimates at certain levels–even for a single state–may require several thousand API queries, and while data can be downloaded in bulk from https://www2.census.gov, you may have to sift through a lot of what you don’t need to get at what you do. It’s also not well-suited for obtaining data in-script.\ncendat aims to address the programmatic need while leveraging automatic query building and concurrency to quickly and easily explore the API’s data offerings, and pull in the data you need. There are other Census API wrapper libraries available for Python, and I honestly haven’t assessed their capabilities more than superficially, but I like building my own tools, so here we are!\ncendat can be installed with pip\npip install cendat\nand you can also install pandas and/or polars at the same time to enable optional methods to work with acquired data\npip install cendat[pandas]\npip install cendat[polars]\npip install cendat[all]\nYou can check out its documentation at https://pypi.org/project/cendat/.\nYou need an API key to get the most out of cendat–you can get one here: https://api.census.gov/data/key_signup.html."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#workflow",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#workflow",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Workflow",
    "text": "Workflow\ncendat includes two classes: CenDatHelper for exploring the API, locking in product, vintage, variable, and geographic selections, and getting data, and CenDatResponse to represent the returned data structure and provide methods for conversion to pandas or polars DataFrames.\nThese classes can be used if you already know exactly what you want and how to specify it, but they also streamline the process of figuring out and selecting what you want through a consistent list \\(\\rightarrow\\) set two-step that works for products, geographies, and variables. Once selections are locked in, the get_data() method builds the queries, issues them concurrently, and organizes the results into a digestible format."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-1.-import-and-instantiate",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-1.-import-and-instantiate",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 1. Import and Instantiate",
    "text": "Step 1. Import and Instantiate\nA CenDatHelper object can be instantiated without argument, or with the key parameter (as shown below) and/or years, which can be provided as an integer or list of integers. API key and years can also be provided later via the load_key() and set_years() methods, respectively. To start exploring, we’ll provide a key but not specify any years of interest.\n\nfrom cendat import CenDatHelper\nfrom dotenv import load_dotenv\nimport os\nfrom pprint import pprint\nimport polars as pl\nload_dotenv()\n\ncdh = CenDatHelper(key=os.getenv('CENSUS_API_KEY'))\n\n✅ API key loaded successfully."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-2.-explore-available-products",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-2.-explore-available-products",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 2. Explore Available Products",
    "text": "Step 2. Explore Available Products\ncendat (currently, as of ver. 0.2.2) supports all aggregate and microdata products available in the API (I’m open to adding timeseries product handling as well, if anyone requests it). To explore those products, we use the list_products() method. Let’s start by looking for anything related to the ACS 5-year detailed tables.\n\npotential_products = cdh.list_products(\n    patterns=[\"acs\", \"5-year\", \"detailed\"]\n)\n\nfor product in potential_products:\n    print(product['title'])\n\nACS 5-Year Detailed Tables (2010/acs/acs5)\nACS 5-Year Detailed Tables (2011/acs/acs5)\nACS 5-Year Detailed Tables (2012/acs/acs5)\nACS 5-Year Detailed Tables (2013/acs/acs5)\nACS 5-Year Detailed Tables (2014/acs/acs5)\nACS 5-Year Detailed Tables (2015/acs/acs5)\nACS 5-Year Detailed Tables (2016/acs/acs5)\nACS 5-Year Detailed Tables (2017/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2018/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2009/acs/acs5)\nACS 5-Year AIAN Detailed Tables (2015/acs/acs5/aian)\nAmerican Community Survey: 5-Year Estimates: Selected Population Detailed Tables 5-Year (2015/acs/acs5/spt)\nAmerican Community Survey: 5-Year Estimates: American Indian and Alaska Native Detailed Tables 5-Year (2010/acs/acs5/aian)\nAmerican Community Survey: 5-Year Estimates: Selected Population Detailed Tables 5-Year (2010/acs/acs5/spt)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2019/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2020/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2021/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: American Indian and Alaska Native Detailed Tables 5-Year (2021/acs/acs5/aian)\nAmerican Community Survey: 5-Year Estimates: Selected Population Detailed Tables 5-Year (2021/acs/acs5/spt)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2022/acs/acs5)\nACS 5-Year Detailed Tables (2023/acs/acs5)\n\n\nOkay, that gives us a good starting point. We’ll get to filtering those results down in a moment, but first I want to highlight a couple of things about the code block above. First, you’ll notice that I provided patterns to filter the products by via the patterns parameter. patterns takes a string or list of strings compiled into (case-insensitive) regular expressions. We can control whether the patterns all have to be met (the default) or only one or more through the logic parameter. To employ any logic set logic=any. If you don’t speak regex, fear not–just supply substrings you’d like to match. We can also control which portion of the metadata the patterns will be used on. The default (which we’ve used here) is to operate on the titles. The alternative is to operate on the descriptions, which can be achieved by setting match_in='desc'. One other point. You’ll notice that I selectively printed only product['title']. By default, the list_products() method returns a list of dictionaries of which the title is only one item (we can force it to return only the title by setting to_dicts=False). We’ll see some more of what’s returned after we filter our products down a bit more.\nOkay, back to the results. A couple of things are obvious: the detailed tables products aren’t named consistently across years, and there are some special population products that are being picked up as well. We can also see that there’s a parenthetical portion to each product title that looks like a directory path–that’s parsed from the JSON packet returned by the API and tacked on by list_products(). So, rather than trying to refine our patterns to capture the inconsistent portion of the titles, we can focus on that and try again.\n\npotential_products = cdh.list_products(\n    patterns=[r\"acs/acs5\\)\"]\n)\n\nfor product in potential_products:\n    print(product['title'])\n\nACS 5-Year Detailed Tables (2010/acs/acs5)\nACS 5-Year Detailed Tables (2011/acs/acs5)\nACS 5-Year Detailed Tables (2012/acs/acs5)\nACS 5-Year Detailed Tables (2013/acs/acs5)\nACS 5-Year Detailed Tables (2014/acs/acs5)\nACS 5-Year Detailed Tables (2015/acs/acs5)\nACS 5-Year Detailed Tables (2016/acs/acs5)\nACS 5-Year Detailed Tables (2017/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2018/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2009/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2019/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2020/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2021/acs/acs5)\nAmerican Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2022/acs/acs5)\nACS 5-Year Detailed Tables (2023/acs/acs5)\n\n\nNow we’ve filtered down to the core product we want, but let’s subset to only a couple years to keep things simple. We’ll also take a look at the full contents of the products’ dictionaries.\n\npotential_products = cdh.list_products(\n    patterns=[r\"acs/acs5\\)\"],\n    years=[2022, 2023]\n)\n\nfor product in potential_products:\n    pprint(product)\n\n{'desc': 'The American Community Survey (ACS) is an ongoing survey that '\n         'provides data every year -- giving communities the current '\n         'information they need to plan investments and services. The ACS '\n         'covers a broad range of topics about social, economic, demographic, '\n         'and housing characteristics of the U.S. population. Summary files '\n         'include the following geographies: nation, all states (including DC '\n         'and Puerto Rico), all metropolitan areas, all congressional '\n         'districts, all counties, all places, and all tracts and block '\n         'groups. Summary files contain the most detailed cross-tabulations, '\n         'many of which are published down to block groups. The data are '\n         'population and housing counts. There are over 64,000 variables in '\n         'this dataset.',\n 'is_aggregate': True,\n 'is_microdata': False,\n 'title': 'American Community Survey: 5-Year Estimates: Detailed Tables 5-Year '\n          '(2022/acs/acs5)',\n 'type': 'acs/acs5',\n 'url': 'http://api.census.gov/data/2022/acs/acs5',\n 'vintage': [2022]}\n{'desc': 'The American Community Survey (ACS) is an ongoing survey that '\n         'provides data every year -- giving communities the current '\n         'information they need to plan investments and services. The ACS '\n         'covers a broad range of topics about social, economic, demographic, '\n         'and housing characteristics of the U.S. population. Summary files '\n         'include the following geographies: nation, all states (including DC '\n         'and Puerto Rico), all metropolitan areas, all congressional '\n         'districts, all counties, all places, and all tracts and block '\n         'groups. Summary files contain the most detailed cross-tabulations, '\n         'many of which are published down to block groups. The data are '\n         'population and housing counts. There are over 64,000 variables in '\n         'this dataset.',\n 'is_aggregate': True,\n 'is_microdata': False,\n 'title': 'ACS 5-Year Detailed Tables (2023/acs/acs5)',\n 'type': 'acs/acs5',\n 'url': 'http://api.census.gov/data/2023/acs/acs5',\n 'vintage': [2023]}\n\n\nOkay, great–now we’ve got what we want, let’s lock in our selection.\n\ncdh.set_products()\n\n✅ Product set: 'American Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2022/acs/acs5)' (Vintage: [2022])\n✅ Product set: 'ACS 5-Year Detailed Tables (2023/acs/acs5)' (Vintage: [2023])"
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-3.-explore-variables",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-3.-explore-variables",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 3. Explore Variables",
    "text": "Step 3. Explore Variables\nNow, let’s start to think about which variables we want–according to the descriptions above, we have over 64,000 to choose from! Let’s assume we’re interested in median household income and average household size. We’ll start by printing out the full dictionary for only the first variable, just to get a sense of the information available to us.\n\npotential_variables = cdh.list_variables(\n    patterns=[\"median household income\", \"household size\"],\n    logic=any\n)\n\n# look at the first to get a sense of what the full dictionaries have to offer\npprint(potential_variables[0])\n\n{'concept': 'Average Household Size of Occupied Housing Units by Tenure '\n            '(Hispanic or Latino Householder)',\n 'group': 'B25010I',\n 'label': 'Estimate!!Average household size --!!Total:!!Owner occupied',\n 'name': 'B25010I_002E',\n 'product': 'American Community Survey: 5-Year Estimates: Detailed Tables '\n            '5-Year (2022/acs/acs5)',\n 'sugg_wgt': 'N/A',\n 'type': 'float',\n 'url': 'http://api.census.gov/data/2022/acs/acs5',\n 'values': 'N/A',\n 'vintage': [2022]}\n\n\nWhen comparing the full dictionary of the first variable to just its label, we can see that there are nuances not captured in the label alone. When we print out the name and label for all found variables, we can see there is duplication in the label due to the lost nuance.\n\n# print only the titles for all of the variables\nfor variable in potential_variables:\n    print(variable['name'], variable['label'])\n\nB25010I_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010I_001E Estimate!!Average household size --!!Total:\nB25010I_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010_001E Estimate!!Average household size --!!Total:\nB19013E_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB19013D_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB22008_002E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)--!!Total:!!Household received Food Stamps/SNAP in the past 12 months\nB22008_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)--!!Total:\nB19013I_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB22008_003E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)--!!Total:!!Household did not receive Food Stamps/SNAP in the past 12 months\nB25010D_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010D_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010D_001E Estimate!!Average household size --!!Total:\nB19013A_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB19013F_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB25010F_001E Estimate!!Average household size --!!Total:\nB25010F_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010F_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010A_001E Estimate!!Average household size --!!Total:\nB25010A_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010A_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010E_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010E_001E Estimate!!Average household size --!!Total:\nB25010E_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25119_002E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Owner occupied (dollars)\nB25119_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:\nB25119_003E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Renter occupied (dollars)\nB19013G_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB25010B_001E Estimate!!Average household size --!!Total:\nB25010B_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010B_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25099_003E Estimate!!Median household income --!!Total:!!Median household income for units without a mortgage\nB25099_001E Estimate!!Median household income --!!Total:\nB25099_002E Estimate!!Median household income --!!Total:!!Median household income for units with a mortgage\nB25010G_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010G_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010G_001E Estimate!!Average household size --!!Total:\nB19013B_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB25010H_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010H_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010H_001E Estimate!!Average household size --!!Total:\nB29004_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB19049_004E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Householder 45 to 64 years\nB19049_005E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Householder 65 years and over\nB19049_002E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Householder under 25 years\nB19049_003E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:!!Householder 25 to 44 years\nB19049_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars) --!!Total:\nB19013C_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB19013_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB25010C_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010C_001E Estimate!!Average household size --!!Total:\nB25010C_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB19013H_001E Estimate!!Median household income in the past 12 months (in 2022 inflation-adjusted dollars)\nB25010I_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010I_001E Estimate!!Average household size --!!Total:\nB25010I_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010_001E Estimate!!Average household size --!!Total:\nB19013E_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB19013D_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB22008_002E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)--!!Total:!!Household received Food Stamps/SNAP in the past 12 months\nB22008_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)--!!Total:\nB19013I_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB22008_003E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)--!!Total:!!Household did not receive Food Stamps/SNAP in the past 12 months\nB25010D_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010D_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010D_001E Estimate!!Average household size --!!Total:\nB19013A_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB19013F_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB25010F_001E Estimate!!Average household size --!!Total:\nB25010F_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010F_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010A_001E Estimate!!Average household size --!!Total:\nB25010A_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010A_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010E_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010E_001E Estimate!!Average household size --!!Total:\nB25010E_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25119_002E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Owner occupied (dollars)\nB25119_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:\nB25119_003E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Renter occupied (dollars)\nB19013G_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB25010B_001E Estimate!!Average household size --!!Total:\nB25010B_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010B_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25099_003E Estimate!!Median household income --!!Total:!!Median household income for units without a mortgage\nB25099_001E Estimate!!Median household income --!!Total:\nB25099_002E Estimate!!Median household income --!!Total:!!Median household income for units with a mortgage\nB25010G_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010G_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010G_001E Estimate!!Average household size --!!Total:\nB19013B_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB25010H_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB25010H_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010H_001E Estimate!!Average household size --!!Total:\nB29004_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB19049_004E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Householder 45 to 64 years\nB19049_005E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Householder 65 years and over\nB19049_002E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Householder under 25 years\nB19049_003E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:!!Householder 25 to 44 years\nB19049_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars) --!!Total:\nB19013C_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB19013_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\nB25010C_002E Estimate!!Average household size --!!Total:!!Owner occupied\nB25010C_001E Estimate!!Average household size --!!Total:\nB25010C_003E Estimate!!Average household size --!!Total:!!Renter occupied\nB19013H_001E Estimate!!Median household income in the past 12 months (in 2023 inflation-adjusted dollars)\n\n\nIn our next pass we’ll filter to promising variables based on label, but we’ll print the variable name and concept, as that seems to capture the nuance we need to be aware of.\n\npotential_variables = cdh.list_variables(\n    patterns=[\n        r\"Average household size --!!Total:$\",\n        r\"Median household income in the past 12 months \\(in 202\\d inflation-adjusted dollars\\)$\"\n    ],\n    logic=any\n)\n\nfor variable in potential_variables:\n    print(variable['name'], \"\\n\", variable['concept'])\n\nB25010I_001E \n Average Household Size of Occupied Housing Units by Tenure (Hispanic or Latino Householder)\nB25010_001E \n Average Household Size of Occupied Housing Units by Tenure\nB19013E_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Native Hawaiian and Other Pacific Islander Alone Householder)\nB19013D_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Asian Alone Householder)\nB19013I_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Hispanic or Latino Householder)\nB25010D_001E \n Average Household Size of Occupied Housing Units by Tenure (Asian Alone Householder)\nB19013A_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (White Alone Householder)\nB19013F_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Some Other Race Alone Householder)\nB25010F_001E \n Average Household Size of Occupied Housing Units by Tenure (Some Other Race Alone Householder)\nB25010A_001E \n Average Household Size of Occupied Housing Units by Tenure (White Alone Householder)\nB25010E_001E \n Average Household Size of Occupied Housing Units by Tenure (Native Hawaiian and Other Pacific Islander Alone Householder)\nB19013G_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Two or More Races Householder)\nB25010B_001E \n Average Household Size of Occupied Housing Units by Tenure (Black or African American Alone Householder)\nB25010G_001E \n Average Household Size of Occupied Housing Units by Tenure (Two or More Races Householder)\nB19013B_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (Black or African American Alone Householder)\nB25010H_001E \n Average Household Size of Occupied Housing Units by Tenure (White Alone, Not Hispanic or Latino Householder)\nB29004_001E \n Median Household Income for Households With a Citizen, Voting-Age Householder (in 2022 Inflation-Adjusted Dollars)\nB19013C_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (American Indian and Alaska Native Alone Householder)\nB19013_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars)\nB25010C_001E \n Average Household Size of Occupied Housing Units by Tenure (American Indian and Alaska Native Alone Householder)\nB19013H_001E \n Median Household Income in the Past 12 Months (in 2022 Inflation-Adjusted Dollars) (White Alone, Not Hispanic or Latino Householder)\nB25010I_001E \n Average Household Size of Occupied Housing Units by Tenure (Hispanic or Latino Householder)\nB25010_001E \n Average Household Size of Occupied Housing Units by Tenure\nB19013E_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Native Hawaiian and Other Pacific Islander Alone Householder)\nB19013D_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Asian Alone Householder)\nB19013I_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Hispanic or Latino Householder)\nB25010D_001E \n Average Household Size of Occupied Housing Units by Tenure (Asian Alone Householder)\nB19013A_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (White Alone Householder)\nB19013F_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Some Other Race Alone Householder)\nB25010F_001E \n Average Household Size of Occupied Housing Units by Tenure (Some Other Race Alone Householder)\nB25010A_001E \n Average Household Size of Occupied Housing Units by Tenure (White Alone Householder)\nB25010E_001E \n Average Household Size of Occupied Housing Units by Tenure (Native Hawaiian and Other Pacific Islander Alone Householder)\nB19013G_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Two or More Races Householder)\nB25010B_001E \n Average Household Size of Occupied Housing Units by Tenure (Black or African American Alone Householder)\nB25010G_001E \n Average Household Size of Occupied Housing Units by Tenure (Two or More Races Householder)\nB19013B_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (Black or African American Alone Householder)\nB25010H_001E \n Average Household Size of Occupied Housing Units by Tenure (White Alone, Not Hispanic or Latino Householder)\nB29004_001E \n Median Household Income for Households With a Citizen, Voting-Age Householder (in 2023 Inflation-Adjusted Dollars)\nB19013C_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (American Indian and Alaska Native Alone Householder)\nB19013_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars)\nB25010C_001E \n Average Household Size of Occupied Housing Units by Tenure (American Indian and Alaska Native Alone Householder)\nB19013H_001E \n Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars) (White Alone, Not Hispanic or Latino Householder)\n\n\nBased on this output we can see that we’re interested in B25010_001E and B19013_001E–we can set these variables explicitly.\n\ncdh.set_variables(\n    names=[\"B25010_001E\", \"B19013_001E\"]\n)\n\n✅ Variables set:\n  - Product: American Community Survey: 5-Year Estimates: Detailed Tables 5-Year (2022/acs/acs5) (Vintage: [2022])\n    Variables: B25010_001E, B19013_001E\n  - Product: ACS 5-Year Detailed Tables (2023/acs/acs5) (Vintage: [2023])\n    Variables: B25010_001E, B19013_001E"
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-4.-explore-geographic-summary-levels",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-4.-explore-geographic-summary-levels",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 4. Explore Geographic Summary Levels",
    "text": "Step 4. Explore Geographic Summary Levels\nNext we need to select the geographic level at which we want to get our estimates. Let’s assume we’re interested in block groups and filter accordingly. That should give us a small number of options, so we’ll output and view the complete dictionaries.\n\npotential_geos = cdh.list_geos(\n    patterns=\"block group\",\n    to_dicts=True\n)\n\npprint(potential_geos)\n\n[{'desc': 'block group',\n  'product': 'American Community Survey: 5-Year Estimates: Detailed Tables '\n             '5-Year (2022/acs/acs5)',\n  'requires': ['state', 'county', 'tract'],\n  'sumlev': '150',\n  'url': 'http://api.census.gov/data/2022/acs/acs5',\n  'vintage': [2022]},\n {'desc': 'tribal block group',\n  'product': 'American Community Survey: 5-Year Estimates: Detailed Tables '\n             '5-Year (2022/acs/acs5)',\n  'requires': ['american indian area/alaska native area/hawaiian home land',\n               'tribal census tract'],\n  'sumlev': '258',\n  'url': 'http://api.census.gov/data/2022/acs/acs5',\n  'vintage': [2022]},\n {'desc': 'tribal block group (or part)',\n  'product': 'American Community Survey: 5-Year Estimates: Detailed Tables '\n             '5-Year (2022/acs/acs5)',\n  'requires': ['american indian area/alaska native area (reservation or '\n               'statistical entity only)',\n               'tribal census tract (or part)'],\n  'sumlev': '293',\n  'url': 'http://api.census.gov/data/2022/acs/acs5',\n  'vintage': [2022]},\n {'desc': 'tribal block group (or part)',\n  'product': 'American Community Survey: 5-Year Estimates: Detailed Tables '\n             '5-Year (2022/acs/acs5)',\n  'requires': ['american indian area (off-reservation trust land '\n               'only)/hawaiian home land',\n               'tribal census tract (or part)'],\n  'sumlev': '294',\n  'url': 'http://api.census.gov/data/2022/acs/acs5',\n  'vintage': [2022]},\n {'desc': 'block group',\n  'product': 'ACS 5-Year Detailed Tables (2023/acs/acs5)',\n  'requires': ['state', 'county', 'tract'],\n  'sumlev': '150',\n  'url': 'http://api.census.gov/data/2023/acs/acs5',\n  'vintage': [2023]},\n {'desc': 'tribal block group',\n  'product': 'ACS 5-Year Detailed Tables (2023/acs/acs5)',\n  'requires': ['american indian area/alaska native area/hawaiian home land',\n               'tribal census tract'],\n  'sumlev': '258',\n  'url': 'http://api.census.gov/data/2023/acs/acs5',\n  'vintage': [2023]},\n {'desc': 'tribal block group (or part)',\n  'product': 'ACS 5-Year Detailed Tables (2023/acs/acs5)',\n  'requires': ['american indian area/alaska native area (reservation or '\n               'statistical entity only)',\n               'tribal census tract (or part)'],\n  'sumlev': '293',\n  'url': 'http://api.census.gov/data/2023/acs/acs5',\n  'vintage': [2023]},\n {'desc': 'tribal block group (or part)',\n  'product': 'ACS 5-Year Detailed Tables (2023/acs/acs5)',\n  'requires': ['american indian area (off-reservation trust land '\n               'only)/hawaiian home land',\n               'tribal census tract (or part)'],\n  'sumlev': '294',\n  'url': 'http://api.census.gov/data/2023/acs/acs5',\n  'vintage': [2023]}]\n\n\nA couple of things to note here. First, we can see in the first dictionary that it is the one we’re interested in (summary level 150). Second, the dictionary includes 'requires': ['state', 'county', 'tract'] which indicates that in order to get estimates for a set of block groups from the API, the query must specify the parent state, county, and tract. So, we will need multiple API calls–one for every tract, each of which also has to provide a specific state and county."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-5.-get-data",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-5.-get-data",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 5. Get Data",
    "text": "Step 5. Get Data\nNotably, there are nearly 84,000 tracts in continental U.S.! To keep this example a little more bite-sized, we’ll focus on just three counties in Colorado and the entire state of Wyoming (mostly to illustrate the flexibility we have in providing nesting information in within). Now, that’s still a lot of tracts, but CenDatHelper does the hard work of figuring out all of the state-county-tract combinations and building/issuing the necessary API calls. First we set our summary level of interest, then we get the data. Let’s see how that works.\n\n%%time\n#| results: markup\n\ncdh.set_geos(\"150\")\n\nresponse = cdh.get_data(\n    within=[\n        {\"state\": \"08\", \"county\": [\"123\", \"013\"]},\n        {\"state\": \"08\", \"county\": \"069\", \"tract\": [\"001307\", \"001810\",\"001308\"]},\n        {\"state\": \"56\"}\n    ]\n)\n\n✅ Geographies set: 'block group' (requires `within` for: county, state, tract)\n✅ Parameters created for 2 geo-variable combinations.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 83 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 78 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 160 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 83 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 78 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 1 combinations for 'block group' within the specified scope.\nℹ️ Fetching parent geographies for 'block group'...\n✅ Found 160 combinations for 'block group' within the specified scope.\nℹ️ Making 648 API call(s)...\nCPU times: total: 328 ms\nWall time: 17.9 s\n\n\nWe can see that in the end 648 API calls were needed to satisfy our request, but due to the wonders of thread pooling, it didn’t take very long at all. Note that this example illustrates the flexibility of within pretty well–we can specify parent geographies as a list of dictionaries at different levels of the geographic hierarchy above the summary level of interest. Since we’re pulling block groups, we can provide state, state and county, or state, county and tract. Within each dictionary, the last item’s values may be provided as a list. We see that above for counties in the first dictionary and tracts in the second. Technically, any item’s values can be provided as a list, we will just issue some API calls that won’t work as intended (we’ll get, say, counties from two different states because they have the same county code) or that won’t work at all (we’ll end up looking for parent geography combinations that don’t exist), and we’ll see messages to that effect in the output. For predictable output, it’s best to stick to lists only in the last item of a given dictionary."
  },
  {
    "objectID": "posts/2025-08-13-Introducing-cendat/index.html#step-6.-convert",
    "href": "posts/2025-08-13-Introducing-cendat/index.html#step-6.-convert",
    "title": "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API",
    "section": "Step 6. Convert",
    "text": "Step 6. Convert\nOur final step is to convert our output to a more friendly format.\n\ndf = pl.concat(\n        response.to_polars(\n            schema_overrides={\n                \"B25010_001E\": pl.Float64, \n                \"B19013_001E\": pl.Float64,\n            }\n    )\n)\n\nprint(df.head())\n\nshape: (5, 10)\n┌─────────────┬─────────────┬───────┬────────┬───┬────────────────┬─────────┬────────┬─────────────┐\n│ B25010_001E ┆ B19013_001E ┆ state ┆ county ┆ … ┆ product        ┆ vintage ┆ sumlev ┆ desc        │\n│ ---         ┆ ---         ┆ ---   ┆ ---    ┆   ┆ ---            ┆ ---     ┆ ---    ┆ ---         │\n│ f64         ┆ f64         ┆ str   ┆ str    ┆   ┆ str            ┆ i32     ┆ str    ┆ str         │\n╞═════════════╪═════════════╪═══════╪════════╪═══╪════════════════╪═════════╪════════╪═════════════╡\n│ 3.21        ┆ 137059.0    ┆ 08    ┆ 123    ┆ … ┆ American       ┆ 2022    ┆ 150    ┆ block group │\n│             ┆             ┆       ┆        ┆   ┆ Community      ┆         ┆        ┆             │\n│             ┆             ┆       ┆        ┆   ┆ Survey: 5-Y…   ┆         ┆        ┆             │\n│ 2.87        ┆ 39868.0     ┆ 08    ┆ 123    ┆ … ┆ American       ┆ 2022    ┆ 150    ┆ block group │\n│             ┆             ┆       ┆        ┆   ┆ Community      ┆         ┆        ┆             │\n│             ┆             ┆       ┆        ┆   ┆ Survey: 5-Y…   ┆         ┆        ┆             │\n│ 1.8         ┆ 37813.0     ┆ 08    ┆ 123    ┆ … ┆ American       ┆ 2022    ┆ 150    ┆ block group │\n│             ┆             ┆       ┆        ┆   ┆ Community      ┆         ┆        ┆             │\n│             ┆             ┆       ┆        ┆   ┆ Survey: 5-Y…   ┆         ┆        ┆             │\n│ 3.4         ┆ 78724.0     ┆ 08    ┆ 123    ┆ … ┆ American       ┆ 2022    ┆ 150    ┆ block group │\n│             ┆             ┆       ┆        ┆   ┆ Community      ┆         ┆        ┆             │\n│             ┆             ┆       ┆        ┆   ┆ Survey: 5-Y…   ┆         ┆        ┆             │\n│ 4.66        ┆ -6.6667e8   ┆ 08    ┆ 123    ┆ … ┆ American       ┆ 2022    ┆ 150    ┆ block group │\n│             ┆             ┆       ┆        ┆   ┆ Community      ┆         ┆        ┆             │\n│             ┆             ┆       ┆        ┆   ┆ Survey: 5-Y…   ┆         ┆        ┆             │\n└─────────────┴─────────────┴───────┴────────┴───┴────────────────┴─────────┴────────┴─────────────┘\n\n\nHere I’ve chosen to convert to polars DataFrames (the response is a list with entries for each product vintage, so to_polars() generates a list of DataFrames - pl.concat() stacks them). I’ve also forced my estimate variables to float type, as they come from the API as strings.\nThat’s all for now! Development is ongoing, and I plan to add new features as they occur to me. Don’t hesitate to reach out of there’s anything you’d like to see (or if you spot any bugs)."
  },
  {
    "objectID": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html",
    "href": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html",
    "title": "Building a Technical Trading Data & Analytics Pipeline",
    "section": "",
    "text": "As a person with a strong background in analytics and a love of programming, I’ve always wanted to have a go at technical investing, using my own pipeline. I’ve finally taken the project on in earnest, and I’m going to chronicle the twists and turns it takes on the blog—this is Part 1.\nSo, what do I mean by ‘pipeline’ in this context? Essentially, I mean ingesting market data, algorithmically curating buy candidates, tracking existing positions for sell signals, and all the nitty gritty in-betweens that entails. I envision four high-level components:\n\nData ingestion and transformation,\nApplication of a model to identify and rank buy candidates,\nDaily, automated report creation to help me make decisions on buy candidates, and\nDaily/intraday reporting/monitoring for existing positions.\n\nI’ve started on numbers 1 and 3, so I will cover some of that here."
  },
  {
    "objectID": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#background",
    "href": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#background",
    "title": "Building a Technical Trading Data & Analytics Pipeline",
    "section": "",
    "text": "As a person with a strong background in analytics and a love of programming, I’ve always wanted to have a go at technical investing, using my own pipeline. I’ve finally taken the project on in earnest, and I’m going to chronicle the twists and turns it takes on the blog—this is Part 1.\nSo, what do I mean by ‘pipeline’ in this context? Essentially, I mean ingesting market data, algorithmically curating buy candidates, tracking existing positions for sell signals, and all the nitty gritty in-betweens that entails. I envision four high-level components:\n\nData ingestion and transformation,\nApplication of a model to identify and rank buy candidates,\nDaily, automated report creation to help me make decisions on buy candidates, and\nDaily/intraday reporting/monitoring for existing positions.\n\nI’ve started on numbers 1 and 3, so I will cover some of that here."
  },
  {
    "objectID": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#data-ingestion",
    "href": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#data-ingestion",
    "title": "Building a Technical Trading Data & Analytics Pipeline",
    "section": "Data Ingestion",
    "text": "Data Ingestion\nI’m going to focus exclusively on stocks to start, and I’ll be getting my data from Polygon.io—they have a variety of data offerings across various personal and business tiers (including a free option). I’ll be using the Stocks Starter plan, which provides a decent amount of historical data aggregated in flat files by day or minute via Amazon S3 as well as near-real-time data via API.\nMy core data object that will serve as input to the curation model will be a Python (Polars) DataFrame of daily aggregates for all U.S. stocks (~10K) over a flexible window of time through the prior trading day. I’ll build the DataFrame from flat files using the Python Boto3 SDK for S3 and two custom functions.\n\nFunction 1: List Files\nThis function returns a list of file names satisfying parameterized criteria (day vs. minute, last day, window size, etc.).\n\ndef list_hist_files(\n        kind='day_aggs',\n        last_day='2025-03-28',\n        window=30,\n        prefix='us_stocks_sip',\n        bucket_name='flatfiles',\n        bookend=False,\n    ):\n\n    session=boto3.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )\n\n    s3=session.client(\n        's3',\n        endpoint_url='https://files.polygon.io',\n        config=Config(signature_version='s3v4'),\n    )\n\n    paginator=s3.get_paginator('list_objects_v2')\n\n    dates=[]\n    end_date=datetime.strptime(last_day,'%Y-%m-%d')\n    for delta in range(window+1):\n        temp_past_date=end_date-timedelta(days=delta)\n        dates.append(datetime.strftime(temp_past_date,'%Y-%m-%d'))\n\n    files=[]\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n        for obj in page['Contents']:\n            if obj['Key'].find(kind)&gt;=0 and re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',obj['Key']) in dates: \n                files.append(obj['Key'])\n\n    if bookend and len(files)&gt;2:\n        files=[files[0],files[-1]]\n\n    return files\n\n\n\nFunction 2: Ingest Files\nThe second function reads a single, dated file for the full market or for an optional subset of tickers into memory and returns a Polars DataFrame. This function has a simple positional parameterization, as it’s intended to be called via the itertools.starmap() functional.\n\ndef get_hist_data(file,tickers):\n\n    date=re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',file)\n\n    session=boto3.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )\n\n    s3=session.client(\n        's3',\n        endpoint_url='https://files.polygon.io',\n        config=Config(signature_version='s3v4'),\n    )\n\n    response=s3.get_object(Bucket='flatfiles',Key=file)\n    compressed_data=response[\"Body\"].read()\n\n    with gzip.GzipFile(fileobj=io.BytesIO(compressed_data),mode=\"rb\") as f:\n        if tickers: df=pl.scan_csv(f).filter(pl.col('ticker').is_in(tickers)).collect()\n        else: df=pl.read_csv(f)\n\n    return df.insert_column(1,pl.lit(date).alias(\"date\"))"
  },
  {
    "objectID": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#data-transformation",
    "href": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#data-transformation",
    "title": "Building a Technical Trading Data & Analytics Pipeline",
    "section": "Data Transformation",
    "text": "Data Transformation\nNext, I pull in the data and create some metrics, including candlesticks, short and long simple moving averages, moving average convergence/divergence (MACD) indicator and signal (its moving average), and the relative strength index (RSI). Note that I convert to a Pandas DataFrame here to make the data readable in a subsequent R block—that’s just a byproduct of this post being written in Quarto. This portion of the pipeline will be scripted when implemented.\n\nimport polars as pl\nimport itertools as it\nimport boto3\nfrom botocore.config import Config\nimport tempfile\nimport gzip\nimport io\nimport re\nfrom datetime import datetime, timedelta\nfrom polygon import RESTClient\nimport polars.selectors as cs\nfrom dataclasses import asdict\n\nwith open(\"/Users/lance/Desktop/TechInvest/scripts/sandbox/01_GetHistAggs.py\") as script:\n    exec(script.read())\n\nwith open(\"/Users/lance/Desktop/TechInvest/keys.py\") as script:\n    exec(script.read())\n\niterator=it.product(\n    list_hist_files(kind=\"day_aggs\",last_day=r.params[\"ref_date\"],window=r.params[\"window\"],bookend=False),\n    [[r.params[\"ticker\"]]],\n)\n\ndf=it.starmap(get_hist_data,iterator)\n\ndf=(\n    pl.concat(list(df))\n    .lazy()\n    .sort(\"ticker\",\"window_start\")\n    .with_columns(\n        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_l\"]).over(\"ticker\").alias(\"sma_l\"),\n        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_s\"]).over(\"ticker\").alias(\"sma_s\"),\n        (pl.col(\"close\").ewm_mean(span=12,min_samples=12)-\n            pl.col(\"close\").ewm_mean(span=26,min_samples=26)\n        ).over(\"ticker\").alias(\"MACD\"),\n        (pl.col(\"close\")*2-pl.col(\"close\").rolling_sum(window_size=2)).over(\"ticker\").alias(\"rsi_diff\"),\n        pl.when(pl.col(\"close\")&gt;pl.col(\"open\")).then(1)\n        .otherwise(-1)\n        .alias(\"candle_color\"),\n        pl.max_horizontal(\"open\",\"close\").alias(\"candle_high\"),\n        pl.min_horizontal(\"open\",\"close\").alias(\"candle_low\"),\n        pl.mean_horizontal(\"open\",\"close\").alias(\"candle_mid\"),\n    )\n    .with_columns(\n        pl.col(\"MACD\").ewm_mean(span=9,min_samples=9).over(\"ticker\").alias(\"signal\"),\n        pl.when(pl.col(\"rsi_diff\")&gt;0).then(\"rsi_diff\")\n        .otherwise(0)\n        .alias(\"U\"),\n        pl.when(pl.col(\"rsi_diff\")&lt;0).then(-pl.col(\"rsi_diff\"))\n        .otherwise(0)\n        .alias(\"D\"),\n    )\n    .with_columns(\n        (pl.col(\"MACD\")-pl.col(\"signal\")).alias(\"histogram\"),\n        ((pl.col(\"U\").ewm_mean(min_samples=14,alpha=1/14))/(pl.col(\"D\").ewm_mean(min_samples=14,alpha=1/14))).alias(\"RS\"),\n    )\n    .with_columns((100-100/(1+pl.col(\"RS\"))).alias(\"RSI\"))\n    .filter(pl.col(\"signal\").is_not_null())\n    .collect()\n    .to_pandas()\n)"
  },
  {
    "objectID": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#a-graphics-template-for-buy-candidate-analysis-and-position-monitoring",
    "href": "posts/2025-04-01-Tech-Invest-Pipeline-Part1/index.html#a-graphics-template-for-buy-candidate-analysis-and-position-monitoring",
    "title": "Building a Technical Trading Data & Analytics Pipeline",
    "section": "A Graphics Template for Buy Candidate Analysis and Position Monitoring",
    "text": "A Graphics Template for Buy Candidate Analysis and Position Monitoring\nI could probably develop these graphics in Python, but I’ve just got way too much ggplot experience at this point and would rather do this part in R. The idea is to present a consistent set of metrics I can use to make buy/sell decisions. I think these will be embedded in ticker-specific html reports along with other relevant information as of yet undetermined. Here’s the code with some example output for NVIDIA stock over a 10-week window ending on April 1, 2025.\n\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(patchwork)\nlibrary(ggthemes)\n\nbg&lt;-\"#7AD151FF\"\nr&lt;-\"#31688EFF\"\n\nhotline&lt;-\"#FDE725FF\"\ncoldline&lt;-\"#1F988BFF\"\n\ntheme&lt;-theme_set(theme_solarized(light=FALSE))+\n    theme_update(\n        axis.text.x=element_blank(),axis.ticks.x=element_blank(),\n        axis.text.y=element_text(color=\"#bbbbbb\")\n    )\n\ng2&lt;-ggplot(py$df,aes(x=as_date(date)))+\n    geom_hline(yintercept=0,linewidth=.5,color=\"#111111\")+\n    geom_col(aes(y=histogram,fill=MACD&gt;signal),color=\"#111111\")+\n    scale_x_date(\n        limits=c(min(as_date(py$df$date))-1,max(as_date(py$df$date))+1),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=0),\n    )+\n    scale_y_continuous()+\n    scale_fill_manual(values=c(r,bg))+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")\n\nu&lt;-layer_scales(g2)$x$limits[[2]] %&gt;% as_date()\nl&lt;-layer_scales(g2)$x$limits[[1]] %&gt;% as_date()\n\ng1&lt;-ggplot(py$df,aes(x=as_date(date)))+\n    geom_hline(yintercept=0,linetype=2,linewidth=.5,color=\"#bbbbbb\")+\n    geom_line(aes(y=signal),color=coldline,linewidth=1.1)+\n    geom_line(aes(y=MACD),color=hotline,linewidth=1.1)+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=0),\n    )+\n    scale_y_continuous()+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")\n\ng3&lt;-ggplot(py$df,aes(x=as_date(date)))+\n    theme_update(\n        axis.text.x=element_text(angle=45,hjust=1,vjust=1,color=\"#bbbbbb\"),\n        axis.ticks.x=element_line(),\n    )+\n    geom_line(aes(y=RSI),linewidth=1.1,color=hotline)+\n    annotate(\n        geom=\"rect\",\n        fill=coldline,\n        xmin=l,\n        xmax=u,\n        ymin=30,\n        ymax=70,\n        alpha=0.5,\n    )+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=0),\n    )+\n    scale_y_continuous(limits=c(0,100),breaks=c(0,30,70,100))+\n    labs(y=NULL,x=NULL)+\n    guides(color=\"none\",fill=\"none\")+\n    theme_update(\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n    )\n\np&lt;-ggplot(py$df,aes(x=as_date(date)))+\n    geom_line(aes(y=sma_l),color=coldline,linewidth=1.1)+\n    geom_line(aes(y=sma_s),color=hotline,linewidth=1.1)+\n    geom_linerange(aes(ymax=high,ymin=low,color=factor(candle_color)),linewidth=1.1)+\n    geom_tile(\n        aes(\n            y=candle_mid,\n            height=candle_high-candle_low,\n            fill=factor(candle_color),\n        ),\n        width=.8,\n        linewidth=.4,\n        color=\"#111111\",\n    )+\n    scale_x_date(\n        limits=c(l,u),\n        date_breaks=\"1 week\",\n        date_minor_breaks=\"1 day\",\n        expand=expansion(add=0),\n    )+\n    scale_fill_manual(values=c(r,bg))+\n    scale_color_manual(values=c(r,bg))+\n    scale_y_continuous(labels=scales::dollar)+\n    labs(y=NULL,x=NULL)+\n    ggtitle(str_glue(\"Ticker: {params$ticker}\"))+\n    guides(color=\"none\",fill=\"none\")\n\np / g1 / g2 / g3 + plot_layout(nrow=4,heights=c(3,1,1,1))\n\n\n\n\n\n\n\n\nMuch more to come, but that’s it for now!"
  },
  {
    "objectID": "posts/2025-08-22-cendat-new-features/index.html",
    "href": "posts/2025-08-22-cendat-new-features/index.html",
    "title": "New Features in cendat ver 0.4.1",
    "section": "",
    "text": "Since the introductory post, I’ve added some new features to make cendat more robust, user-friendly, and capable:\n\nThe biggest update is the addition of the tabulate() method for the CenDatResponse objects returned by get_data().\nThe CenDatResponse methods to_polars() and to_pandas() got new functionality\n\nAutomatic concatenation via the new concat parameter, which defaults to False for backward compatibility with prior versions\nDestringing–the Census API returns all data in strings, which prevents polars and pandas from inferring the schema. You can now set destring=True to force type conversion on the raw response data and allow polars and pandas to infer the schema automatically. This parameter defaults to False, and the schema may still be specified by the user via schema_overrides with or without destringing.\n\nThe user can now specify the API timeout count in seconds via timeout in the get_data() method to accommodate longer requests–the default is 30 seconds.\nThe get_data() method gained the preview_only parameter (boolean, defaults to False). If True, the number of API queries required to satisfy the request will be determined and messaged, but the queries will not be executed. This is a handy precursor step when dealing with very granular or unfamiliar geographic summary levels which could yield tens of thousands of queries.\nPattern matching in the list_variables() method has been extended to the “concept” field."
  },
  {
    "objectID": "posts/2025-08-22-cendat-new-features/index.html#quick-acs-aggregate-analysis",
    "href": "posts/2025-08-22-cendat-new-features/index.html#quick-acs-aggregate-analysis",
    "title": "New Features in cendat ver 0.4.1",
    "section": "Quick ACS Aggregate Analysis",
    "text": "Quick ACS Aggregate Analysis\nHere we want to see how many counties in the U.S., as of 2023, had a million or more residents, and how many people lived in those counties.\n\n%%time\n\nfrom cendat import CenDatHelper\nfrom dotenv import load_dotenv\nimport os\nfrom pprint import pprint\nimport polars as pl\nload_dotenv()\n\ncdh = CenDatHelper(key=os.getenv('CENSUS_API_KEY'))\n\ncdh.list_products(years=[2023], patterns=r\"/acs/acs1\\)\")\ncdh.set_products()\ncdh.set_variables(\"B01001_001E\") # total population\ncdh.set_geos(\"050\") # counties\nresponse = cdh.get_data()\n\n# how many counties\nresponse.tabulate(\"state\", where=\"B01001_001E &gt; 1_000_000\")\n\n# how many people in those counties\nresponse.tabulate(\"state\", weight_var=\"B01001_001E\", where=\"B01001_001E &gt; 1_000_000\")\n\n✅ API key loaded successfully.\n✅ Product set: 'ACS 1-Year Detailed Tables (2023/acs/acs1)' (Vintage: [2023])\n✅ Variables set:\n  - Product: ACS 1-Year Detailed Tables (2023/acs/acs1) (Vintage: [2023])\n    Variables: B01001_001E\n✅ Geographies set: 'county' (requires `within` for: state)\n✅ Parameters created for 1 geo-variable combinations.\nℹ️ Fetching parent geographies for 'county'...\n✅ Found 52 combinations for 'county' within the specified scope.\nℹ️ Making 52 API call(s)...\nshape: (18, 5)\n┌───────┬────┬──────┬──────┬────────┐\n│ state ┆  n ┆  pct ┆ cumn ┆ cumpct │\n╞═══════╪════╪══════╪══════╪════════╡\n│    04 ┆  2 ┆  4.2 ┆    2 ┆    4.2 │\n│    06 ┆ 10 ┆ 20.8 ┆   12 ┆   25.0 │\n│    12 ┆  6 ┆ 12.5 ┆   18 ┆   37.5 │\n│    13 ┆  1 ┆  2.1 ┆   19 ┆   39.6 │\n│    17 ┆  1 ┆  2.1 ┆   20 ┆   41.7 │\n│    24 ┆  1 ┆  2.1 ┆   21 ┆   43.8 │\n│    25 ┆  1 ┆  2.1 ┆   22 ┆   45.8 │\n│    26 ┆  2 ┆  4.2 ┆   24 ┆   50.0 │\n│    27 ┆  1 ┆  2.1 ┆   25 ┆   52.1 │\n│    32 ┆  1 ┆  2.1 ┆   26 ┆   54.2 │\n│    36 ┆  6 ┆ 12.5 ┆   32 ┆   66.7 │\n│    37 ┆  2 ┆  4.2 ┆   34 ┆   70.8 │\n│    39 ┆  2 ┆  4.2 ┆   36 ┆   75.0 │\n│    42 ┆  2 ┆  4.2 ┆   38 ┆   79.2 │\n│    48 ┆  7 ┆ 14.6 ┆   45 ┆   93.8 │\n│    49 ┆  1 ┆  2.1 ┆   46 ┆   95.8 │\n│    51 ┆  1 ┆  2.1 ┆   47 ┆   97.9 │\n│    53 ┆  1 ┆  2.1 ┆   48 ┆  100.0 │\n└───────┴────┴──────┴──────┴────────┘\nshape: (18, 5)\n┌───────┬────────────┬──────┬────────────┬────────┐\n│ state ┆          n ┆  pct ┆       cumn ┆ cumpct │\n╞═══════╪════════════╪══════╪════════════╪════════╡\n│    04 ┆  5,649,033 ┆  5.8 ┆  5,649,033 ┆    5.8 │\n│    06 ┆ 28,013,381 ┆ 28.7 ┆ 33,662,414 ┆   34.5 │\n│    12 ┆ 10,221,001 ┆ 10.5 ┆ 43,883,415 ┆   45.0 │\n│    13 ┆  1,079,105 ┆  1.1 ┆ 44,962,520 ┆   46.1 │\n│    17 ┆  5,087,072 ┆  5.2 ┆ 50,049,592 ┆   51.3 │\n│    24 ┆  1,058,474 ┆  1.1 ┆ 51,108,066 ┆   52.4 │\n│    25 ┆  1,623,952 ┆  1.7 ┆ 52,732,018 ┆   54.1 │\n│    26 ┆  3,021,595 ┆  3.1 ┆ 55,753,613 ┆   57.1 │\n│    27 ┆  1,258,713 ┆  1.3 ┆ 57,012,326 ┆   58.4 │\n│    32 ┆  2,336,573 ┆  2.4 ┆ 59,348,899 ┆   60.8 │\n│    36 ┆ 10,672,233 ┆ 10.9 ┆ 70,021,132 ┆   71.8 │\n│    37 ┆  2,353,976 ┆  2.4 ┆ 72,375,108 ┆   74.2 │\n│    39 ┆  2,559,151 ┆  2.6 ┆ 74,934,259 ┆   76.8 │\n│    42 ┆  2,775,367 ┆  2.8 ┆ 77,709,626 ┆   79.7 │\n│    48 ┆ 15,250,132 ┆ 15.6 ┆ 92,959,758 ┆   95.3 │\n│    49 ┆  1,185,813 ┆  1.2 ┆ 94,145,571 ┆   96.5 │\n│    51 ┆  1,141,878 ┆  1.2 ┆ 95,287,449 ┆   97.7 │\n│    53 ┆  2,271,380 ┆  2.3 ┆ 97,558,829 ┆  100.0 │\n└───────┴────────────┴──────┴────────────┴────────┘\nCPU times: user 1.14 s, sys: 178 ms, total: 1.32 s\nWall time: 8.68 s\n\n\nIn 2023 there were 48 counties with populations over a million (mostly in California, Florida, New York, and Texas), and the total population across those counties was nearly 98 million."
  },
  {
    "objectID": "posts/2025-08-22-cendat-new-features/index.html#cps-microdata-tabulation",
    "href": "posts/2025-08-22-cendat-new-features/index.html#cps-microdata-tabulation",
    "title": "New Features in cendat ver 0.4.1",
    "section": "CPS Microdata Tabulation",
    "text": "CPS Microdata Tabulation\nIn this example, we’ll pull microdata from the Current Population Survey’s tobacco supplements for 2022 and 2023, and compare the proportions of established daily smokers across a selection of states. We’ll start with just 2022 to limit the output in our variable search. We’re looking for variables that indicate the respondent has smooked at least 100 cigarettes and is a current every day smoker.\n\ncdh.list_products(years=[2022], patterns=\"/cps/tobacco\")\ncdh.set_products()\nfor var in cdh.list_variables(patterns=[r\"smoke.*every day\", \"100\"], logic=any):\n    print(var['name'], var['label'])\n\n✅ Product set: 'Current Population Survey: Tobacco Use Supplement (2022/cps/tobacco/sep)' (Vintage: [2022])\nPEB7C How long have you smoked every day\nPEC7D How long did you smoke every day\nPEC7A Current some day smokers ever smoke every day for 6 months\nPEA1 Smoked at least 100 cigarettes\nPEA3 Categorize now smoke: every day  some days  not at all.\nPEC8 Current some day smoker was every day  some days  or not at all 12 months ago\nPTC7E Howm many cigs smoked each day when smoking every day\nPEH6 Smoked every day  some days  or not at all\nPEH5 For how long smoked EVERY DAY\n\n\nIt looks like we’re interested in PEA1 and PEA3–we’ll inspect the full variable dictionaries to make sure.\n\ncdh.list_variables(patterns=[\"PEA1\", \"PEA3\"], logic=any, match_in=\"name\")\n\n[{'name': 'PEA1',\n  'label': 'Smoked at least 100 cigarettes',\n  'concept': 'N/A',\n  'group': 'N/A',\n  'values': {'item': {'-2': \"Don't Know\",\n    '2': 'No',\n    '-1': 'Not in universe',\n    '-3': 'Refused',\n    '1': 'Yes',\n    '-9': 'No Answer'}},\n  'type': 'int',\n  'sugg_wgt': 'PWNRWGT',\n  'product': 'Current Population Survey: Tobacco Use Supplement (2022/cps/tobacco/sep)',\n  'vintage': [2022],\n  'url': 'http://api.census.gov/data/2022/cps/tobacco/sep'},\n {'name': 'PEA3',\n  'label': 'Categorize now smoke: every day  some days  not at all.',\n  'concept': 'N/A',\n  'group': 'N/A',\n  'values': {'item': {'2': 'Some_days',\n    '1': 'Every_day',\n    '-2': \"Don't Know\",\n    '3': 'Not_at_all',\n    '-3': 'Refused',\n    '-9': 'No Answer',\n    '-1': 'Not in universe'}},\n  'type': 'int',\n  'sugg_wgt': 'PWNRWGT',\n  'product': 'Current Population Survey: Tobacco Use Supplement (2022/cps/tobacco/sep)',\n  'vintage': [2022],\n  'url': 'http://api.census.gov/data/2022/cps/tobacco/sep'}]\n\n\nThese variables get at what we want, and since we’re interested in the proportion among all adults, we can ignore the out of universe responsdents (likely never smokers that didn’t make it past a gate question… this is just an example, so we don’t need to dig any deeper).\nNote that we can also see the suggested weights to use for analysis of these variables, so we’ll include that in our set variables list. First, though, we’ll expand our products to cover 2023 as well.\n\ncdh.list_products(years=[2022, 2023], patterns=\"/cps/tobacco\")\ncdh.set_products()\ncdh.set_variables([\"PEA1\", \"PEA3\", \"PWNRWGT\"])\n\n✅ Product set: 'Current Population Survey: Tobacco Use Supplement (2022/cps/tobacco/sep)' (Vintage: [2022])\n✅ Product set: 'Current Population Survey: Tobacco Use Supplement (2023/cps/tobacco/jan)' (Vintage: [2023])\n✅ Product set: 'Current Population Survey: Tobacco Use Supplement (2023/cps/tobacco/may)' (Vintage: [2023])\n✅ Variables set:\n  - Product: Current Population Survey: Tobacco Use Supplement (2022/cps/tobacco/sep) (Vintage: [2022])\n    Variables: PEA1, PEA3, PWNRWGT\n  - Product: Current Population Survey: Tobacco Use Supplement (2023/cps/tobacco/jan) (Vintage: [2023])\n    Variables: PEA1, PEA3, PWNRWGT\n  - Product: Current Population Survey: Tobacco Use Supplement (2023/cps/tobacco/may) (Vintage: [2023])\n    Variables: PEA1, PEA3, PWNRWGT\n\n\nNext, we’ll set our geography of interest–states, specified by description (vs sumlev)–and get our data. Since this is a microdata request, we have to explicitly specify the geographies we want, corresponding to the summary level we set in set_geos(). We’ll specify our states in the within argument of get_data(). Let’s assume we’re interested in comparing the proportions between California (06) and Texas (48).\n\ncdh.set_geos(\"state\", \"desc\")\nresponse = cdh.get_data(within={'state': ['06', '48']})\nresponse.tabulate(\n    \"PEA1\", \"PEA3\",\n    strat_by=\"state\",\n    weight_var=\"PWNRWGT\",\n    weight_div=3,\n)\n\n✅ Geographies set: 'state'\n✅ Parameters created for 3 geo-variable combinations.\nℹ️ Making 6 API call(s)...\nshape: (43, 7)\n┌───────┬──────┬──────┬──────────────┬──────┬──────────────┬────────┐\n│ state ┆ PEA1 ┆ PEA3 ┆            n ┆  pct ┆         cumn ┆ cumpct │\n╞═══════╪══════╪══════╪══════════════╪══════╪══════════════╪════════╡\n│    48 ┆   -3 ┆   -3 ┆     12,151.4 ┆  0.1 ┆     12,151.4 ┆    0.1 │\n│    48 ┆   -3 ┆   -2 ┆      7,862.0 ┆  0.0 ┆     20,013.4 ┆    0.1 │\n│    48 ┆   -3 ┆   -1 ┆     31,283.4 ┆  0.1 ┆     51,296.9 ┆    0.2 │\n│    48 ┆   -3 ┆    3 ┆      6,011.1 ┆  0.0 ┆     57,308.0 ┆    0.3 │\n│    48 ┆   -2 ┆   -9 ┆      7,299.8 ┆  0.0 ┆     64,607.8 ┆    0.3 │\n│    48 ┆   -2 ┆   -2 ┆      1,871.9 ┆  0.0 ┆     66,479.6 ┆    0.3 │\n│    48 ┆   -2 ┆   -1 ┆     42,634.2 ┆  0.2 ┆    109,113.9 ┆    0.5 │\n│    48 ┆   -2 ┆    2 ┆      2,698.4 ┆  0.0 ┆    111,812.3 ┆    0.5 │\n│    48 ┆   -2 ┆    3 ┆     11,474.7 ┆  0.1 ┆    123,287.0 ┆    0.6 │\n│    48 ┆   -1 ┆   -1 ┆          0.0 ┆  0.0 ┆    123,287.0 ┆    0.6 │\n│    48 ┆    1 ┆   -9 ┆     12,524.6 ┆  0.1 ┆    135,811.6 ┆    0.6 │\n│    48 ┆    1 ┆   -3 ┆      6,074.9 ┆  0.0 ┆    141,886.5 ┆    0.6 │\n│    48 ┆    1 ┆   -2 ┆      2,451.6 ┆  0.0 ┆    144,338.1 ┆    0.6 │\n│    48 ┆    1 ┆    1 ┆  1,230,269.3 ┆  5.5 ┆  1,374,607.4 ┆    6.2 │\n│    48 ┆    1 ┆    2 ┆    521,851.7 ┆  2.3 ┆  1,896,459.0 ┆    8.5 │\n│    48 ┆    1 ┆    3 ┆  2,993,540.7 ┆ 13.4 ┆  4,889,999.7 ┆   21.9 │\n│    48 ┆    2 ┆   -9 ┆      9,233.3 ┆  0.0 ┆  4,899,233.0 ┆   21.9 │\n│    48 ┆    2 ┆   -1 ┆ 11,019,066.7 ┆ 49.3 ┆ 15,918,299.7 ┆   71.2 │\n│    48 ┆    2 ┆    2 ┆     63,562.7 ┆  0.3 ┆ 15,981,862.4 ┆   71.5 │\n│    48 ┆    2 ┆    3 ┆  6,364,974.8 ┆ 28.5 ┆ 22,346,837.2 ┆  100.0 │\n│     6 ┆   -9 ┆   -1 ┆      2,588.4 ┆  0.0 ┆      2,588.4 ┆    0.0 │\n│     6 ┆   -3 ┆   -3 ┆      4,518.2 ┆  0.0 ┆      7,106.6 ┆    0.0 │\n│     6 ┆   -3 ┆   -1 ┆     38,037.9 ┆  0.1 ┆     45,144.6 ┆    0.2 │\n│     6 ┆   -3 ┆    3 ┆      3,679.8 ┆  0.0 ┆     48,824.4 ┆    0.2 │\n│     6 ┆   -2 ┆   -9 ┆      2,126.6 ┆  0.0 ┆     50,951.0 ┆    0.2 │\n│     6 ┆   -2 ┆   -3 ┆      1,726.2 ┆  0.0 ┆     52,677.2 ┆    0.2 │\n│     6 ┆   -2 ┆   -2 ┆     13,550.3 ┆  0.0 ┆     66,227.5 ┆    0.2 │\n│     6 ┆   -2 ┆   -1 ┆    100,758.4 ┆  0.3 ┆    166,985.9 ┆    0.6 │\n│     6 ┆   -2 ┆    3 ┆     16,578.8 ┆  0.1 ┆    183,564.7 ┆    0.6 │\n│     6 ┆   -1 ┆   -1 ┆          0.0 ┆  0.0 ┆    183,564.7 ┆    0.6 │\n│     6 ┆    1 ┆   -9 ┆     11,301.9 ┆  0.0 ┆    194,866.6 ┆    0.6 │\n│     6 ┆    1 ┆   -3 ┆     14,985.0 ┆  0.0 ┆    209,851.7 ┆    0.7 │\n│     6 ┆    1 ┆   -2 ┆     14,227.3 ┆  0.0 ┆    224,078.9 ┆    0.7 │\n│     6 ┆    1 ┆    1 ┆  1,054,186.4 ┆  3.5 ┆  1,278,265.3 ┆    4.3 │\n│     6 ┆    1 ┆    2 ┆    576,434.2 ┆  1.9 ┆  1,854,699.5 ┆    6.2 │\n│     6 ┆    1 ┆    3 ┆  3,532,156.8 ┆ 11.8 ┆  5,386,856.3 ┆   18.0 │\n│     6 ┆    2 ┆   -9 ┆     18,986.8 ┆  0.1 ┆  5,405,843.1 ┆   18.0 │\n│     6 ┆    2 ┆   -3 ┆      4,104.9 ┆  0.0 ┆  5,409,947.9 ┆   18.0 │\n│     6 ┆    2 ┆   -2 ┆      7,485.7 ┆  0.0 ┆  5,417,433.7 ┆   18.1 │\n│     6 ┆    2 ┆   -1 ┆ 16,344,706.7 ┆ 54.5 ┆ 21,762,140.4 ┆   72.5 │\n│     6 ┆    2 ┆    1 ┆     13,137.2 ┆  0.0 ┆ 21,775,277.6 ┆   72.6 │\n│     6 ┆    2 ┆    2 ┆     74,384.9 ┆  0.2 ┆ 21,849,662.4 ┆   72.8 │\n│     6 ┆    2 ┆    3 ┆  8,153,180.9 ┆ 27.2 ┆ 30,002,843.3 ┆  100.0 │\n└───────┴──────┴──────┴──────────────┴──────┴──────────────┴────────┘\n\n\nWe’re interested in the rows where PEA1 == 1 (have smoked 100+ cigarettes) and PEA3 == 1 (currently smoke every day). Since we’re pooling three supplement waves, we need to divide the weights by 3 to get totals that make sense. We also stratify by state to get results that are comparable across states.\nIn 2022/2023, 5.5% of adults in Texas were estimated to be established, daily cigarette smokers. The figure in California was 3.5%."
  },
  {
    "objectID": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html",
    "href": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html",
    "title": "R vs. Python Query Compute Time Example",
    "section": "",
    "text": "Let’s compare the compute time needed for an equivalent operation between Python and R. The operation is to:\nIn Python, we will use polars with lazy evaluation. In R, we will use dplyr, dtplyr, and tidytable. The latter two packages interpret dplyr syntax and deploy the data.table equivalent for efficiency."
  },
  {
    "objectID": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#plain-dplyr",
    "href": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#plain-dplyr",
    "title": "R vs. Python Query Compute Time Example",
    "section": "Plain dplyr",
    "text": "Plain dplyr\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(hms)\n\n\nAttaching package: 'hms'\n\nThe following object is masked from 'package:lubridate':\n\n    hms\n\nstart&lt;-Sys.time()\n\nrows&lt;-data.table::fread(\"big.csv\") %&gt;%\n  group_by(id) %&gt;%\n  summarize(across(where(is.double),mean),.groups=\"keep\") %&gt;%\n  filter(if_any(where(is.double),~.x&gt;.4)) %&gt;%\n  nrow()\n\nend&lt;-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n\n301 rows returned\nelapsed time for query: 00:01:57.947875"
  },
  {
    "objectID": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#dtplyr",
    "href": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#dtplyr",
    "title": "R vs. Python Query Compute Time Example",
    "section": "dtplyr",
    "text": "dtplyr\nThis is stylistically the R version that is most similar to the polars approach, but it does come with some downsides in that not all dplyr functionality is supported. In this example that is most obvious in the inability to use tidyselect helpers in summarize() and filter().\n\nlibrary(dtplyr,warn.conflicts=F)\n\nstart&lt;-Sys.time()\n\nbig&lt;-data.table::fread(\"big.csv\")\n\nvarnames&lt;-setdiff(colnames(big),\"id\")\n\nrows&lt;-lazy_dt(big) %&gt;%\n  group_by(id) %&gt;%\n  summarize(across(all_of(varnames),mean),.groups=\"keep\") %&gt;%\n  filter(if_any(all_of(varnames),~.x&gt;.4)) %&gt;%\n  collect() %&gt;%\n  nrow()\n\nend&lt;-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n\n301 rows returned\nelapsed time for query: 00:00:43.045766"
  },
  {
    "objectID": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#tidytable",
    "href": "posts/2025-03-19-R-v-Python-Compute-Time-Ex/index.html#tidytable",
    "title": "R vs. Python Query Compute Time Example",
    "section": "tidytable",
    "text": "tidytable\nThis should be computationally comparable to the dtplyr approach as both are deploying data.table behind the scenes, but this approach has the benefit of preserving the plain dplyr syntax, including the ability to use tidyselect helpers.\n\nstart&lt;-Sys.time()\n\nrows&lt;-data.table::fread(\"big.csv\") %&gt;%\n  tidytable::group_by(id) %&gt;%\n  tidytable::summarize(tidytable::across(where(is.double),mean),.groups=\"keep\") %&gt;%\n  tidytable::filter(tidytable::if_any(where(is.double),~.x&gt;.4)) %&gt;%\n  nrow()\n\nend&lt;-Sys.time()\n\nprint(str_glue(\"{rows} rows returned\\nelapsed time for query: {as_hms(end-start)}\"))\n\n301 rows returned\nelapsed time for query: 00:00:56.428901"
  },
  {
    "objectID": "posts/2025-03-20-2-Options-Parameterizing-R-w-Code/index.html",
    "href": "posts/2025-03-20-2-Options-Parameterizing-R-w-Code/index.html",
    "title": "Decoupling Dynamic Code from a Static R Codebase",
    "section": "",
    "text": "Oftentimes the best way to keep code working is to just not touch it. And while even the best, most stable code can’t escape tweaking forever, there are some types of changes that can at least be made without even opening an otherwise stable and static codebase, assuming it’s been set up to allow that.\nSuppose for example that we have an estimation pipeline that runs every year. In most years there are no changes to the methods or the structure of inputs/outputs, but every year there are some unavoidable changes to recode specifications. In this scenario, we have to be able to update the process but ideally in a way that minimizes both the effort required to QC the changes and the probability that something breaks. We can minimize the breakage potential by not opening the code at all, and we can minimize QC time by extracting only the affected code into a parameter file.\nHere are two ways to do that.\n\nOption 1 - A Separate Script\nIn this option, we can store the recode logic in a separate R script. Here we define two new recodes, cyl.rec and mpg.rec based on the mtcars data frame. The rules are stored in vectors with each vector position containing, as strings, individual case_when() conditions and assignments.\n\nlibrary(tidyverse)\n\n#this part can exist in a separate script\nparms&lt;-tribble(\n  ~newvar,     ~rules,\n  \"cyl.rec\",   c(\"cyl==4~1\",\"cyl==6~2\",\"cyl==8~3\"),\n  \"mpg.rec\",   c(\"mpg&lt;15~'very bad'\",\"mpg&lt;20~'bad'\",\"mpg&lt;25~'good'\",\"TRUE~'very good'\")\n)\n\nWe then have a static codebase that walks over the parameter file, creating recodes according to whatever code is found there.\nTo achieve this, we utilize purrr::pwalk() to iterate over the parameter file parms, applying for each row an anonymous function that creates the recode corresponding to that row.\nThe recode is created by injecting parms$newvar as the new variable name, and splicing (via !!!) the vector of conditions from parms$rules into the body of case_when(). Notably, for each iteration, cars is read in from the global environment, the recode is created, and cars is written to the global environment. Alternatively, we could create within the function body a data frame containing only the newly-defined column, capture them across iterations in a list (using purrr::pmap() instead of purrr::pwalk()) and column bind the list along with cars. I’ve done it both ways, but I prefer the global environment overwrite approach used below.\n\n#this part represents a static codebase that would follow a source() call \n# to the parameter file-generating script\n\ncars&lt;-mtcars %&gt;%\n  rownames_to_column(\"car\")\n\npwalk(\n  parms\n  ,function(newvar,rules,df=cars){\n\n    df.name&lt;-deparse(substitute(df))\n    \n    df %&gt;%\n      mutate(!!newvar:=case_when(!!!rlang::parse_exprs(rules))) %&gt;% \n      assign(df.name,.,envir=globalenv())\n    \n  }\n)\n\nselect(cars,car,cyl,cyl.rec,mpg,mpg.rec)\n\n                   car cyl cyl.rec  mpg   mpg.rec\n1            Mazda RX4   6       2 21.0      good\n2        Mazda RX4 Wag   6       2 21.0      good\n3           Datsun 710   4       1 22.8      good\n4       Hornet 4 Drive   6       2 21.4      good\n5    Hornet Sportabout   8       3 18.7       bad\n6              Valiant   6       2 18.1       bad\n7           Duster 360   8       3 14.3  very bad\n8            Merc 240D   4       1 24.4      good\n9             Merc 230   4       1 22.8      good\n10            Merc 280   6       2 19.2       bad\n11           Merc 280C   6       2 17.8       bad\n12          Merc 450SE   8       3 16.4       bad\n13          Merc 450SL   8       3 17.3       bad\n14         Merc 450SLC   8       3 15.2       bad\n15  Cadillac Fleetwood   8       3 10.4  very bad\n16 Lincoln Continental   8       3 10.4  very bad\n17   Chrysler Imperial   8       3 14.7  very bad\n18            Fiat 128   4       1 32.4 very good\n19         Honda Civic   4       1 30.4 very good\n20      Toyota Corolla   4       1 33.9 very good\n21       Toyota Corona   4       1 21.5      good\n22    Dodge Challenger   8       3 15.5       bad\n23         AMC Javelin   8       3 15.2       bad\n24          Camaro Z28   8       3 13.3  very bad\n25    Pontiac Firebird   8       3 19.2       bad\n26           Fiat X1-9   4       1 27.3 very good\n27       Porsche 914-2   4       1 26.0 very good\n28        Lotus Europa   4       1 30.4 very good\n29      Ford Pantera L   8       3 15.8       bad\n30        Ferrari Dino   6       2 19.7       bad\n31       Maserati Bora   8       3 15.0       bad\n32          Volvo 142E   4       1 21.4      good\n\n\n\n\nOption 2 - Code Stored as Text in a Separate File (like a csv)\nOption 2 does the same thing—creating recodes metaprogrammatically by storing the code as data—but may be a better fit if we want to store the code in text-based, tabular format rather than in an R script. This can be useful, for example, if we want someone who is a subject-matter expert but not an R programmer to write or review the recode code (in this case we could even break down the conditions in the parameter file further to strip out the case_when() syntax and reassemble as necessary in the static codebase).\n\n#this part can exist in .csv or .xlsx file\nparms.alt&lt;-tribble(\n  ~newvar,     ~rules,\n  \"cyl.rec\",   \"cyl==4~1\",\n  \"cyl.rec\",   \"cyl==6~2\",\n  \"cyl.rec\",   \"cyl==8~3\",\n  \"mpg.rec\",   \"mpg&lt;15~'very bad'\",\n  \"mpg.rec\",   \"mpg&lt;20~'bad'\",\n  \"mpg.rec\",   \"mpg&lt;25~'good'\",\n  \"mpg.rec\",   \"TRUE~'very good'\"\n) \n\nThe main difference on the static codebase side is that we group the parameter file by newvar and use group_walk() to apply our anonymous function after extracting the rules vector manually.\n\n#this part represents a static codebase that would follow an ingestion step\n# that reads in the parameter file from wherever it's stored\n\ncars&lt;-mtcars %&gt;%\n  rownames_to_column(\"car\")\n\nparms.alt %&gt;%\n  group_by(newvar) %&gt;%\n  group_walk(\n    function(rules,group,df=cars){\n      \n      df.name&lt;-deparse(substitute(df))\n      \n      newvar&lt;-pull(group,newvar)\n      rules&lt;-pull(rules,rules)\n\n      df %&gt;%\n        mutate(!!newvar:=case_when(!!!rlang::parse_exprs(rules))) %&gt;%\n        assign(df.name,.,envir=globalenv())\n      \n    }\n  )\n\nselect(cars,car,cyl,cyl.rec,mpg,mpg.rec)\n\n                   car cyl cyl.rec  mpg   mpg.rec\n1            Mazda RX4   6       2 21.0      good\n2        Mazda RX4 Wag   6       2 21.0      good\n3           Datsun 710   4       1 22.8      good\n4       Hornet 4 Drive   6       2 21.4      good\n5    Hornet Sportabout   8       3 18.7       bad\n6              Valiant   6       2 18.1       bad\n7           Duster 360   8       3 14.3  very bad\n8            Merc 240D   4       1 24.4      good\n9             Merc 230   4       1 22.8      good\n10            Merc 280   6       2 19.2       bad\n11           Merc 280C   6       2 17.8       bad\n12          Merc 450SE   8       3 16.4       bad\n13          Merc 450SL   8       3 17.3       bad\n14         Merc 450SLC   8       3 15.2       bad\n15  Cadillac Fleetwood   8       3 10.4  very bad\n16 Lincoln Continental   8       3 10.4  very bad\n17   Chrysler Imperial   8       3 14.7  very bad\n18            Fiat 128   4       1 32.4 very good\n19         Honda Civic   4       1 30.4 very good\n20      Toyota Corolla   4       1 33.9 very good\n21       Toyota Corona   4       1 21.5      good\n22    Dodge Challenger   8       3 15.5       bad\n23         AMC Javelin   8       3 15.2       bad\n24          Camaro Z28   8       3 13.3  very bad\n25    Pontiac Firebird   8       3 19.2       bad\n26           Fiat X1-9   4       1 27.3 very good\n27       Porsche 914-2   4       1 26.0 very good\n28        Lotus Europa   4       1 30.4 very good\n29      Ford Pantera L   8       3 15.8       bad\n30        Ferrari Dino   6       2 19.7       bad\n31       Maserati Bora   8       3 15.0       bad\n32          Volvo 142E   4       1 21.4      good\n\n\n\nIn either case, the recode changes are easy to QC and we eliminate the chance that we could break stable code by not even having to open it.\n\n\n\n\nCitationBibTeX citation:@online{couzens2025,\n  author = {Couzens, Lance},\n  title = {Decoupling {Dynamic} {Code} from a {Static} {R} {Codebase}},\n  date = {2025-03-20},\n  url = {https://mostlyunoriginal.github.io/posts/2025-03-20-2-Options-Parameterizing-R-w-Code/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCouzens, Lance. 2025. “Decoupling Dynamic Code from a Static R\nCodebase.” March 20, 2025. https://mostlyunoriginal.github.io/posts/2025-03-20-2-Options-Parameterizing-R-w-Code/."
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "",
    "text": "Before diving into the examples below, it’s important to acknowledge some general differences between R and Python and specific differences in style and approach between Tidy- and Polars-style data manipulation. First and foremost: Python has a strong object orientation while R is essentially a functional language. The practical impact of that difference here is that Python objects are manipulated, or their attributes extracted, by way of methods, while R objects are inputs and outputs of functions. But Python uses functions too, and in fact methods are themselves functions, so this can be very confusing!\nWhat is a method, then? In simple terms, it’s a function defined as part of the blueprint for a given type (or ‘class’) of object. A Polars DataFrame is a class of object, and there are certain functions defined in that class—these are the Polars DataFrame methods. By creating a specific DataFrame, we ‘instantiate’ the class into an object, and we can deploy a predefined set of methods to do things with or to that object.\nIn both R and Python we often want to do several operations in a row without distinct assignments for each intermediate step of a process. In R—and especially in the Tidy style of R programming—we can use piping with either the magrittr or base pipes (%&gt;% and |&gt;, respectively) to achieve this. The resulting pipeline starts with an object, passes that object into a function which returns a new object which is passed into another function, and so on and so forth until the desired object is returned by the final function in the pipeline and is captured with an assignment, returned to the console, or passed as input to another pipeline or function. Consider the following example.\n\ncyls&lt;-mtcars %&gt;% #1, 5\n  distinct(cyl) %&gt;% #2\n  arrange(cyl) %&gt;% #3\n  pull() #4\n\nHere, we start with the data frame mtcars (1), which is piped as input to the distinct() function along with the column reference cyl (2), which returns a data frame containing only the column cyl and one row for each distinct value. This is piped as input to arrange() (3) along with a column reference to cyl, which returns a sorted data frame. This is piped into pull() (4), which extracts a single column (the only one there: cyl) as a vector. This final object is then assigned to the environment variable cyls (5). Now consider the Python version which utilizes a technique called ‘method chaining’.\n\ncyls=( #5\n    mtcars #1\n    .unique(\"cyl\") #2\n    .sort(\"cyl\") #3\n    .get_column(\"cyl\") #4\n)\n\nHere, we start with mtcars, a Polars DataFrame (1). We then apply the unique() method with a reference to the column cyl (2), yielding a Polars DataFrame containing the distinct values of cyl (note that it still contains all the other variables too!). Calling the sort() method sorts the rows by the values of cyl (3). The Polars DataFrame method get_column() (4) extracts a single column and yields a Polars Series (analogous to the atomic vectors that comprise R data frame columns). The resulting Series is assigned to the variable cyls (5).\nBoth of these code blocks look quite similar, and the Python version should feel familiar to anyone who employs the Tidy-style of programming in R. Now that we’ve seen method chaining in action we can introduce a twist that unlocks some additional efficiency and that may seem strange compared to the Tidy style. The Python block above utilizes what’s called ‘eager evaluation’, which means the code inside cyls=(...) is immediately evaluated and in exactly the manner we have specified. However, Polars is actually implemented in Rust (a high performance systems programming language) and has a query optimization capability that we can exploit via something called ‘lazy evaluation’. The following ‘lazy’ alternative to the previous example gathers our instructions, performs query optimization (yielding a ‘query plan’), and ultimately executes an optimized query only when we invoke the collect() method (a method of Polars LazyFrames which result from invoking the lazy() method of a regular Polars DataFrame).\n\ncyls=(\n    mtcars\n    .lazy()\n    .unique(\"cyl\")\n    .sort(\"cyl\")\n    .collect()\n    .get_column(\"cyl\")\n)\n\nNote that Polars LazyFrames do not have a get_column() method like DataFrames do—it can therefore only be invoked after collection. The advantages of lazy evaluation in this trivial example would not be noticeable but could be significant depending on the size of the data and the complexity of the query. Lazy evaluation also allows for efficient processing of larger-than-memory data frames. See the User guide for more detail. This approach may seem familiar to anyone who has used the dtplyr R package which allows the user to proved dplyr syntax which is translated into data.table (which is written primarily in C and is much faster than dplyr) under the hood.\nWithout further ado, let’s dive into some examples."
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#background",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#background",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "",
    "text": "Before diving into the examples below, it’s important to acknowledge some general differences between R and Python and specific differences in style and approach between Tidy- and Polars-style data manipulation. First and foremost: Python has a strong object orientation while R is essentially a functional language. The practical impact of that difference here is that Python objects are manipulated, or their attributes extracted, by way of methods, while R objects are inputs and outputs of functions. But Python uses functions too, and in fact methods are themselves functions, so this can be very confusing!\nWhat is a method, then? In simple terms, it’s a function defined as part of the blueprint for a given type (or ‘class’) of object. A Polars DataFrame is a class of object, and there are certain functions defined in that class—these are the Polars DataFrame methods. By creating a specific DataFrame, we ‘instantiate’ the class into an object, and we can deploy a predefined set of methods to do things with or to that object.\nIn both R and Python we often want to do several operations in a row without distinct assignments for each intermediate step of a process. In R—and especially in the Tidy style of R programming—we can use piping with either the magrittr or base pipes (%&gt;% and |&gt;, respectively) to achieve this. The resulting pipeline starts with an object, passes that object into a function which returns a new object which is passed into another function, and so on and so forth until the desired object is returned by the final function in the pipeline and is captured with an assignment, returned to the console, or passed as input to another pipeline or function. Consider the following example.\n\ncyls&lt;-mtcars %&gt;% #1, 5\n  distinct(cyl) %&gt;% #2\n  arrange(cyl) %&gt;% #3\n  pull() #4\n\nHere, we start with the data frame mtcars (1), which is piped as input to the distinct() function along with the column reference cyl (2), which returns a data frame containing only the column cyl and one row for each distinct value. This is piped as input to arrange() (3) along with a column reference to cyl, which returns a sorted data frame. This is piped into pull() (4), which extracts a single column (the only one there: cyl) as a vector. This final object is then assigned to the environment variable cyls (5). Now consider the Python version which utilizes a technique called ‘method chaining’.\n\ncyls=( #5\n    mtcars #1\n    .unique(\"cyl\") #2\n    .sort(\"cyl\") #3\n    .get_column(\"cyl\") #4\n)\n\nHere, we start with mtcars, a Polars DataFrame (1). We then apply the unique() method with a reference to the column cyl (2), yielding a Polars DataFrame containing the distinct values of cyl (note that it still contains all the other variables too!). Calling the sort() method sorts the rows by the values of cyl (3). The Polars DataFrame method get_column() (4) extracts a single column and yields a Polars Series (analogous to the atomic vectors that comprise R data frame columns). The resulting Series is assigned to the variable cyls (5).\nBoth of these code blocks look quite similar, and the Python version should feel familiar to anyone who employs the Tidy-style of programming in R. Now that we’ve seen method chaining in action we can introduce a twist that unlocks some additional efficiency and that may seem strange compared to the Tidy style. The Python block above utilizes what’s called ‘eager evaluation’, which means the code inside cyls=(...) is immediately evaluated and in exactly the manner we have specified. However, Polars is actually implemented in Rust (a high performance systems programming language) and has a query optimization capability that we can exploit via something called ‘lazy evaluation’. The following ‘lazy’ alternative to the previous example gathers our instructions, performs query optimization (yielding a ‘query plan’), and ultimately executes an optimized query only when we invoke the collect() method (a method of Polars LazyFrames which result from invoking the lazy() method of a regular Polars DataFrame).\n\ncyls=(\n    mtcars\n    .lazy()\n    .unique(\"cyl\")\n    .sort(\"cyl\")\n    .collect()\n    .get_column(\"cyl\")\n)\n\nNote that Polars LazyFrames do not have a get_column() method like DataFrames do—it can therefore only be invoked after collection. The advantages of lazy evaluation in this trivial example would not be noticeable but could be significant depending on the size of the data and the complexity of the query. Lazy evaluation also allows for efficient processing of larger-than-memory data frames. See the User guide for more detail. This approach may seem familiar to anyone who has used the dtplyr R package which allows the user to proved dplyr syntax which is translated into data.table (which is written primarily in C and is much faster than dplyr) under the hood.\nWithout further ado, let’s dive into some examples."
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-1.-basic-summarize-without-generalization-across-variables",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-1.-basic-summarize-without-generalization-across-variables",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 1. Basic Summarize without Generalization across Variables",
    "text": "Example 1. Basic Summarize without Generalization across Variables\nHere, we take on a very simple and very common task: calculating the mean of a continuous variable (mpg) by the levels of a categorical variable (cyl).\n\nR Version\nThe Tidy approach utilizes a pipeline comprised of the mtcars data frame and the group_by() and summarize() functions. Note that these functions take a data frame (or tibble) as the first argument, but prevailing style allows this to be passed implicitly (as is done here).\n\nlibrary(dplyr)\n\ntable&lt;-mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    summarize(mpg.mean=mean(mpg))\n\nprint(table)\n\n# A tibble: 3 × 2\n    cyl mpg.mean\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     4     26.7\n2     6     19.7\n3     8     15.1\n\n\n\n\nPython Version\nThe Polars approach below begins by reading the R mtcars data frame into the Polars LazyFrame mtcars. The LazyFrame method group_by() is invoked followed by the agg() method. agg() contains an expression that is itself a method chain which yields the mean values for each group as the new variable mpg.mean. table is a Polars DataFrame realized as the result of evaluating an optimized query plan (via collect()).\n\nimport polars as pl\n\nmtcars=pl.LazyFrame(r.mtcars)\n\nq=(\n    mtcars\n    .group_by(\"cyl\")\n    .agg(pl.col(\"mpg\").mean().alias(\"mpg.mean\"))\n)\n\ntable=q.collect()\n\nprint(table)\n\nshape: (3, 2)\n┌─────┬───────────┐\n│ cyl ┆ mpg.mean  │\n│ --- ┆ ---       │\n│ f64 ┆ f64       │\n╞═════╪═══════════╡\n│ 8.0 ┆ 15.1      │\n│ 6.0 ┆ 19.742857 │\n│ 4.0 ┆ 26.663636 │\n└─────┴───────────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-2.-basic-mutate-with-grouping-and-without-generalization",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-2.-basic-mutate-with-grouping-and-without-generalization",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 2. Basic Mutate with Grouping and without Generalization",
    "text": "Example 2. Basic Mutate with Grouping and without Generalization\nHere we want to add a new variable to our data frame—the new variable is the ratio of each value of mpg relative to the mean value for the group (defined by the levels of the variable cyl).\n\nR Version\nIn R I can create the new variable with a call to mutate() that utilizes both group-level statistics and record-level data. This can be done in a single step with very little code.\n\ntable&lt;-mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    mutate(rel.mpg=mpg/mean(mpg))\n\nprint(table)\n\n# A tibble: 32 × 12\n# Groups:   cyl [3]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb rel.mpg\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4   1.06 \n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4   1.06 \n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1   0.855\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1   1.08 \n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2   1.24 \n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1   0.917\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4   0.947\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2   0.915\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2   0.855\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4   0.973\n# ℹ 22 more rows\n\n\n\n\nPython Version\nThe Python version uses the with_columns() LazyFrame method. Here, unlike in the R version, the grouping is baked into the recode expression itself by way of over(). Aside from looking a bit different, the Polars approach is more powerful because each expression can utilize its own grouping. Note that the Polars documentation utilizes a ‘contexts’ and ‘expressions’ framework to describe what could also be referred to as methods or method chains. In this example, with_columns() is the context in which the expression yielding the new variable rel.mpg is nested.\n\nq=(\n    mtcars\n    .with_columns(\n        (pl.col(\"mpg\")/pl.col(\"mpg\").mean().over(\"cyl\")).alias(\"rel.mpg\")\n    )\n)\n\ntable=q.collect()\n\nprint(table)\n\nshape: (32, 12)\n┌──────┬─────┬───────┬───────┬───┬─────┬──────┬──────┬──────────┐\n│ mpg  ┆ cyl ┆ disp  ┆ hp    ┆ … ┆ am  ┆ gear ┆ carb ┆ rel.mpg  │\n│ ---  ┆ --- ┆ ---   ┆ ---   ┆   ┆ --- ┆ ---  ┆ ---  ┆ ---      │\n│ f64  ┆ f64 ┆ f64   ┆ f64   ┆   ┆ f64 ┆ f64  ┆ f64  ┆ f64      │\n╞══════╪═════╪═══════╪═══════╪═══╪═════╪══════╪══════╪══════════╡\n│ 21.0 ┆ 6.0 ┆ 160.0 ┆ 110.0 ┆ … ┆ 1.0 ┆ 4.0  ┆ 4.0  ┆ 1.063676 │\n│ 21.0 ┆ 6.0 ┆ 160.0 ┆ 110.0 ┆ … ┆ 1.0 ┆ 4.0  ┆ 4.0  ┆ 1.063676 │\n│ 22.8 ┆ 4.0 ┆ 108.0 ┆ 93.0  ┆ … ┆ 1.0 ┆ 4.0  ┆ 1.0  ┆ 0.855097 │\n│ 21.4 ┆ 6.0 ┆ 258.0 ┆ 110.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 1.0  ┆ 1.083936 │\n│ 18.7 ┆ 8.0 ┆ 360.0 ┆ 175.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 2.0  ┆ 1.238411 │\n│ …    ┆ …   ┆ …     ┆ …     ┆ … ┆ …   ┆ …    ┆ …    ┆ …        │\n│ 30.4 ┆ 4.0 ┆ 95.1  ┆ 113.0 ┆ … ┆ 1.0 ┆ 5.0  ┆ 2.0  ┆ 1.14013  │\n│ 15.8 ┆ 8.0 ┆ 351.0 ┆ 264.0 ┆ … ┆ 1.0 ┆ 5.0  ┆ 4.0  ┆ 1.046358 │\n│ 19.7 ┆ 6.0 ┆ 145.0 ┆ 175.0 ┆ … ┆ 1.0 ┆ 5.0  ┆ 6.0  ┆ 0.997829 │\n│ 15.0 ┆ 8.0 ┆ 301.0 ┆ 335.0 ┆ … ┆ 1.0 ┆ 5.0  ┆ 8.0  ┆ 0.993377 │\n│ 21.4 ┆ 4.0 ┆ 121.0 ┆ 109.0 ┆ … ┆ 1.0 ┆ 4.0  ┆ 2.0  ┆ 0.802591 │\n└──────┴─────┴───────┴───────┴───┴─────┴──────┴──────┴──────────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-3.-summarize-generalized-by-variable-type-with-across",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-3.-summarize-generalized-by-variable-type-with-across",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 3. Summarize Generalized by Variable Type with Across",
    "text": "Example 3. Summarize Generalized by Variable Type with Across\nIn this example we want to generate means by group (like in example 1), but across a set of columns described by a selection criteria (i.e., not by name).\n\nR Version\nAs before, we specify the grouping via group_by() and generate the means within summarize(). In order to create means for several variables not explicitly specified we can utilize across(). To get means for all variables stored as doubles, we use the helper function where() in the .cols specification. Glue syntax in the .names specification allows us to rename all affected columns.\n\nmtcars %&gt;%\n    group_by(cyl,gear) %&gt;%\n    summarize(\n        across(\n            .cols=where(is.double)\n            ,.fns=mean\n            ,.names=\"{.col}_mean\"\n        )\n    )\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 11\n# Groups:   cyl [3]\n    cyl  gear mpg_mean disp_mean hp_mean drat_mean wt_mean qsec_mean vs_mean\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     4     3     21.5      120.     97       3.7     2.46      20.0     1  \n2     4     4     26.9      103.     76       4.11    2.38      19.6     1  \n3     4     5     28.2      108.    102       4.1     1.83      16.8     0.5\n4     6     3     19.8      242.    108.      2.92    3.34      19.8     1  \n5     6     4     19.8      164.    116.      3.91    3.09      17.7     0.5\n6     6     5     19.7      145     175       3.62    2.77      15.5     0  \n7     8     3     15.0      358.    194.      3.12    4.10      17.1     0  \n8     8     5     15.4      326     300.      3.88    3.37      14.6     0  \n# ℹ 2 more variables: am_mean &lt;dbl&gt;, carb_mean &lt;dbl&gt;\n\n\n\n\nPython Version\nThe Python version looks very similar to the R version and has the same basic structure as in example 1. Here, though, instead of specifying a column with pl.col() we indicate that we want all columns stored as floats by using cs.float(). Note that there are many selector functions available, as explained here. The name method name.suffix() is used to rename all affected variables. See other name methods here.\n\nimport polars.selectors as cs\n\nq=(\n    mtcars\n    .group_by(\"cyl\",\"gear\")\n    .agg(cs.float().mean().name.suffix(\"_mean\"))\n)\n\ntable=q.collect()\n\nprint(table)\n\nshape: (8, 11)\n┌─────┬──────┬──────────┬────────────┬───┬───────────┬─────────┬─────────┬───────────┐\n│ cyl ┆ gear ┆ mpg_mean ┆ disp_mean  ┆ … ┆ qsec_mean ┆ vs_mean ┆ am_mean ┆ carb_mean │\n│ --- ┆ ---  ┆ ---      ┆ ---        ┆   ┆ ---       ┆ ---     ┆ ---     ┆ ---       │\n│ f64 ┆ f64  ┆ f64      ┆ f64        ┆   ┆ f64       ┆ f64     ┆ f64     ┆ f64       │\n╞═════╪══════╪══════════╪════════════╪═══╪═══════════╪═════════╪═════════╪═══════════╡\n│ 8.0 ┆ 3.0  ┆ 15.05    ┆ 357.616667 ┆ … ┆ 17.1425   ┆ 0.0     ┆ 0.0     ┆ 3.083333  │\n│ 4.0 ┆ 3.0  ┆ 21.5     ┆ 120.1      ┆ … ┆ 20.01     ┆ 1.0     ┆ 0.0     ┆ 1.0       │\n│ 4.0 ┆ 4.0  ┆ 26.925   ┆ 102.625    ┆ … ┆ 19.6125   ┆ 1.0     ┆ 0.75    ┆ 1.5       │\n│ 6.0 ┆ 3.0  ┆ 19.75    ┆ 241.5      ┆ … ┆ 19.83     ┆ 1.0     ┆ 0.0     ┆ 1.0       │\n│ 4.0 ┆ 5.0  ┆ 28.2     ┆ 107.7      ┆ … ┆ 16.8      ┆ 0.5     ┆ 1.0     ┆ 2.0       │\n│ 8.0 ┆ 5.0  ┆ 15.4     ┆ 326.0      ┆ … ┆ 14.55     ┆ 0.0     ┆ 1.0     ┆ 6.0       │\n│ 6.0 ┆ 4.0  ┆ 19.75    ┆ 163.8      ┆ … ┆ 17.67     ┆ 0.5     ┆ 0.5     ┆ 4.0       │\n│ 6.0 ┆ 5.0  ┆ 19.7     ┆ 145.0      ┆ … ┆ 15.5      ┆ 0.0     ┆ 1.0     ┆ 6.0       │\n└─────┴──────┴──────────┴────────────┴───┴───────────┴─────────┴─────────┴───────────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-4.-conditional-recode",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-4.-conditional-recode",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 4. Conditional Recode",
    "text": "Example 4. Conditional Recode\nIn this example we use if/else if/else logic to create a string recode of the numeric variable mpg.\n\nR Version\nIn the Tidy R approach we deploy case_when() inside of mutate() to create a recode with cascading conditional logic.\n\nmtcars %&gt;%\n    mutate(\n        mpg.cat=case_when(\n            mpg&lt;10~\"very bad\"\n            ,mpg&lt;15~\"bad\"\n            ,mpg&lt;20~\"okay\"\n            ,mpg&lt;25~\"good\"\n            ,TRUE~\"great\"\n        )\n    ) %&gt;%\n    arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb mpg.cat\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1   great\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1   great\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2   great\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2   great\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1   great\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2   great\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2    good\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1    good\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2    good\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1    good\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1    good\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2    good\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4    good\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4    good\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6    okay\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4    okay\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2    okay\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2    okay\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1    okay\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4    okay\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3    okay\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3    okay\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4    okay\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2    okay\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3    okay\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2    okay\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8    okay\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4     bad\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4     bad\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4     bad\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4     bad\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4     bad\n\n\n\n\nPython Version\nThe Python version is quite a bit more wordy. Note that pl.lit() is needed here to resolve ambiguity in the way column references can appear as strings in then()—in other words, we’re indicating we want the recoded values to be the provided strings, not the values of columns represented by those strings.\n\nq=(\n    mtcars\n    .with_columns(\n        pl.when(pl.col(\"mpg\")&lt;10).then(pl.lit(\"very bad\"))\n        .when(pl.col(\"mpg\")&lt;15).then(pl.lit(\"bad\"))\n        .when(pl.col(\"mpg\")&lt;20).then(pl.lit(\"okay\"))\n        .when(pl.col(\"mpg\")&lt;25).then(pl.lit(\"good\"))\n        .otherwise(pl.lit(\"great\"))\n        .alias(\"mpg.cat\")\n    )\n    .sort(\"mpg\",descending=True)\n)\n\ndf=q.collect()\n\nprint(df)\n\nshape: (32, 12)\n┌──────┬─────┬───────┬───────┬───┬─────┬──────┬──────┬─────────┐\n│ mpg  ┆ cyl ┆ disp  ┆ hp    ┆ … ┆ am  ┆ gear ┆ carb ┆ mpg.cat │\n│ ---  ┆ --- ┆ ---   ┆ ---   ┆   ┆ --- ┆ ---  ┆ ---  ┆ ---     │\n│ f64  ┆ f64 ┆ f64   ┆ f64   ┆   ┆ f64 ┆ f64  ┆ f64  ┆ str     │\n╞══════╪═════╪═══════╪═══════╪═══╪═════╪══════╪══════╪═════════╡\n│ 33.9 ┆ 4.0 ┆ 71.1  ┆ 65.0  ┆ … ┆ 1.0 ┆ 4.0  ┆ 1.0  ┆ great   │\n│ 32.4 ┆ 4.0 ┆ 78.7  ┆ 66.0  ┆ … ┆ 1.0 ┆ 4.0  ┆ 1.0  ┆ great   │\n│ 30.4 ┆ 4.0 ┆ 75.7  ┆ 52.0  ┆ … ┆ 1.0 ┆ 4.0  ┆ 2.0  ┆ great   │\n│ 30.4 ┆ 4.0 ┆ 95.1  ┆ 113.0 ┆ … ┆ 1.0 ┆ 5.0  ┆ 2.0  ┆ great   │\n│ 27.3 ┆ 4.0 ┆ 79.0  ┆ 66.0  ┆ … ┆ 1.0 ┆ 4.0  ┆ 1.0  ┆ great   │\n│ …    ┆ …   ┆ …     ┆ …     ┆ … ┆ …   ┆ …    ┆ …    ┆ …       │\n│ 14.7 ┆ 8.0 ┆ 440.0 ┆ 230.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 4.0  ┆ bad     │\n│ 14.3 ┆ 8.0 ┆ 360.0 ┆ 245.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 4.0  ┆ bad     │\n│ 13.3 ┆ 8.0 ┆ 350.0 ┆ 245.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 4.0  ┆ bad     │\n│ 10.4 ┆ 8.0 ┆ 472.0 ┆ 205.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 4.0  ┆ bad     │\n│ 10.4 ┆ 8.0 ┆ 460.0 ┆ 215.0 ┆ … ┆ 0.0 ┆ 3.0  ┆ 4.0  ┆ bad     │\n└──────┴─────┴───────┴───────┴───┴─────┴──────┴──────┴─────────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-5.-pivots",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-5.-pivots",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 5. Pivots",
    "text": "Example 5. Pivots\nIn this example we start with mtcars (a version with rownames mapped to the column car), pivot to a long file and then back to wide.\n\nR Version\nHere we use very simple forms of pivot_longer() and pivot_wider().\n\nlibrary(tidyr)\nlibrary(tibble)\n\ncars&lt;-rownames_to_column(mtcars,\"car\") %&gt;%\n  pivot_longer(\n    cols=where(is.numeric)\n    ,names_to=\"variable\"\n    ,values_to=\"value\"\n  )\n\nprint(cars)\n\n# A tibble: 352 × 3\n   car       variable  value\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 Mazda RX4 mpg       21   \n 2 Mazda RX4 cyl        6   \n 3 Mazda RX4 disp     160   \n 4 Mazda RX4 hp       110   \n 5 Mazda RX4 drat       3.9 \n 6 Mazda RX4 wt         2.62\n 7 Mazda RX4 qsec      16.5 \n 8 Mazda RX4 vs         0   \n 9 Mazda RX4 am         1   \n10 Mazda RX4 gear       4   \n# ℹ 342 more rows\n\nmtcars_w_names&lt;-cars %&gt;%\n  pivot_wider(\n    id_cols=car\n    ,names_from=\"variable\"\n    ,values_from=\"value\"\n  )\n\nprint(mtcars_w_names)\n\n# A tibble: 32 × 12\n   car           mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\n\n\nPython Version\nWith polars, going from wide to long is an unpivot and long to wide is a pivot. Note that the pivot() is only available in eager mode, as shown below.\n\nimport polars.selectors as cs\n\nq=(\n    pl.scan_csv(\"mtcars_w_names.csv\")\n    .unpivot(\n        on=cs.numeric()\n        ,index=\"car\"\n        ,variable_name=\"variable\"\n        ,value_name=\"value\"\n    )\n)\n\ncars=q.collect()\n\nprint(cars)\n\nshape: (352, 3)\n┌───────────────────┬──────────┬───────┐\n│ car               ┆ variable ┆ value │\n│ ---               ┆ ---      ┆ ---   │\n│ str               ┆ str      ┆ f64   │\n╞═══════════════════╪══════════╪═══════╡\n│ Mazda RX4         ┆ mpg      ┆ 21.0  │\n│ Mazda RX4 Wag     ┆ mpg      ┆ 21.0  │\n│ Datsun 710        ┆ mpg      ┆ 22.8  │\n│ Hornet 4 Drive    ┆ mpg      ┆ 21.4  │\n│ Hornet Sportabout ┆ mpg      ┆ 18.7  │\n│ …                 ┆ …        ┆ …     │\n│ Lotus Europa      ┆ carb     ┆ 2.0   │\n│ Ford Pantera L    ┆ carb     ┆ 4.0   │\n│ Ferrari Dino      ┆ carb     ┆ 6.0   │\n│ Maserati Bora     ┆ carb     ┆ 8.0   │\n│ Volvo 142E        ┆ carb     ┆ 2.0   │\n└───────────────────┴──────────┴───────┘\n\nmtcars_w_names=(\n    cars\n    .pivot(\n        index=\"car\"\n        ,on=\"variable\"\n        ,values=\"value\"\n        ,aggregate_function=None\n    )\n)\n\nprint(mtcars_w_names)\n\nshape: (32, 12)\n┌───────────────────┬──────┬─────┬───────┬───┬─────┬─────┬──────┬──────┐\n│ car               ┆ mpg  ┆ cyl ┆ disp  ┆ … ┆ vs  ┆ am  ┆ gear ┆ carb │\n│ ---               ┆ ---  ┆ --- ┆ ---   ┆   ┆ --- ┆ --- ┆ ---  ┆ ---  │\n│ str               ┆ f64  ┆ f64 ┆ f64   ┆   ┆ f64 ┆ f64 ┆ f64  ┆ f64  │\n╞═══════════════════╪══════╪═════╪═══════╪═══╪═════╪═════╪══════╪══════╡\n│ Mazda RX4         ┆ 21.0 ┆ 6.0 ┆ 160.0 ┆ … ┆ 0.0 ┆ 1.0 ┆ 4.0  ┆ 4.0  │\n│ Mazda RX4 Wag     ┆ 21.0 ┆ 6.0 ┆ 160.0 ┆ … ┆ 0.0 ┆ 1.0 ┆ 4.0  ┆ 4.0  │\n│ Datsun 710        ┆ 22.8 ┆ 4.0 ┆ 108.0 ┆ … ┆ 1.0 ┆ 1.0 ┆ 4.0  ┆ 1.0  │\n│ Hornet 4 Drive    ┆ 21.4 ┆ 6.0 ┆ 258.0 ┆ … ┆ 1.0 ┆ 0.0 ┆ 3.0  ┆ 1.0  │\n│ Hornet Sportabout ┆ 18.7 ┆ 8.0 ┆ 360.0 ┆ … ┆ 0.0 ┆ 0.0 ┆ 3.0  ┆ 2.0  │\n│ …                 ┆ …    ┆ …   ┆ …     ┆ … ┆ …   ┆ …   ┆ …    ┆ …    │\n│ Lotus Europa      ┆ 30.4 ┆ 4.0 ┆ 95.1  ┆ … ┆ 1.0 ┆ 1.0 ┆ 5.0  ┆ 2.0  │\n│ Ford Pantera L    ┆ 15.8 ┆ 8.0 ┆ 351.0 ┆ … ┆ 0.0 ┆ 1.0 ┆ 5.0  ┆ 4.0  │\n│ Ferrari Dino      ┆ 19.7 ┆ 6.0 ┆ 145.0 ┆ … ┆ 0.0 ┆ 1.0 ┆ 5.0  ┆ 6.0  │\n│ Maserati Bora     ┆ 15.0 ┆ 8.0 ┆ 301.0 ┆ … ┆ 0.0 ┆ 1.0 ┆ 5.0  ┆ 8.0  │\n│ Volvo 142E        ┆ 21.4 ┆ 4.0 ┆ 121.0 ┆ … ┆ 1.0 ┆ 1.0 ┆ 4.0  ┆ 2.0  │\n└───────────────────┴──────┴─────┴───────┴───┴─────┴─────┴──────┴──────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-6.-joins",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-6.-joins",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 6. Joins",
    "text": "Example 6. Joins\nIn this example we will show the various join types with two distinct but overlapping subsets of mtcars: cars with 6-cylinder engines and those with horsepower less than 110.\n\nR Version\nThis code is pretty self-explanatory.\n\ncarsl&lt;-mtcars %&gt;%\n    rownames_to_column(\"car\") %&gt;%\n    filter(cyl==6) %&gt;%\n    select(car,cyl)\n\ncarsr&lt;-mtcars %&gt;%\n    rownames_to_column(\"car\") %&gt;%\n    filter(hp&lt;110) %&gt;%\n    select(car,hp)\n\nprint(carsl)\n\n             car cyl\n1      Mazda RX4   6\n2  Mazda RX4 Wag   6\n3 Hornet 4 Drive   6\n4        Valiant   6\n5       Merc 280   6\n6      Merc 280C   6\n7   Ferrari Dino   6\n\nprint(carsr)\n\n              car  hp\n1      Datsun 710  93\n2         Valiant 105\n3       Merc 240D  62\n4        Merc 230  95\n5        Fiat 128  66\n6     Honda Civic  52\n7  Toyota Corolla  65\n8   Toyota Corona  97\n9       Fiat X1-9  66\n10  Porsche 914-2  91\n11     Volvo 142E 109\n\nleft_join(carsl,carsr,by=\"car\")\n\n             car cyl  hp\n1      Mazda RX4   6  NA\n2  Mazda RX4 Wag   6  NA\n3 Hornet 4 Drive   6  NA\n4        Valiant   6 105\n5       Merc 280   6  NA\n6      Merc 280C   6  NA\n7   Ferrari Dino   6  NA\n\nright_join(carsl,carsr,by=\"car\")\n\n              car cyl  hp\n1         Valiant   6 105\n2      Datsun 710  NA  93\n3       Merc 240D  NA  62\n4        Merc 230  NA  95\n5        Fiat 128  NA  66\n6     Honda Civic  NA  52\n7  Toyota Corolla  NA  65\n8   Toyota Corona  NA  97\n9       Fiat X1-9  NA  66\n10  Porsche 914-2  NA  91\n11     Volvo 142E  NA 109\n\ninner_join(carsl,carsr,by=\"car\")\n\n      car cyl  hp\n1 Valiant   6 105\n\nfull_join(carsl,carsr,by=\"car\")\n\n              car cyl  hp\n1       Mazda RX4   6  NA\n2   Mazda RX4 Wag   6  NA\n3  Hornet 4 Drive   6  NA\n4         Valiant   6 105\n5        Merc 280   6  NA\n6       Merc 280C   6  NA\n7    Ferrari Dino   6  NA\n8      Datsun 710  NA  93\n9       Merc 240D  NA  62\n10       Merc 230  NA  95\n11       Fiat 128  NA  66\n12    Honda Civic  NA  52\n13 Toyota Corolla  NA  65\n14  Toyota Corona  NA  97\n15      Fiat X1-9  NA  66\n16  Porsche 914-2  NA  91\n17     Volvo 142E  NA 109\n\nanti_join(carsl,carsr,by=\"car\")\n\n             car cyl\n1      Mazda RX4   6\n2  Mazda RX4 Wag   6\n3 Hornet 4 Drive   6\n4       Merc 280   6\n5      Merc 280C   6\n6   Ferrari Dino   6\n\n\n\n\nPython Version\n\ncarsl=(\n    pl.scan_csv(\"mtcars_w_names.csv\")\n    .filter(pl.col(\"cyl\")==6)\n    .select(\"car\",\"cyl\")\n)\n\ncarsr=(\n    pl.scan_csv(\"mtcars_w_names.csv\")\n    .filter(pl.col(\"hp\")&lt;110)\n    .select(\"car\",\"hp\")\n)\n\nprint(carsl.collect())\n\nshape: (7, 2)\n┌────────────────┬─────┐\n│ car            ┆ cyl │\n│ ---            ┆ --- │\n│ str            ┆ i64 │\n╞════════════════╪═════╡\n│ Mazda RX4      ┆ 6   │\n│ Mazda RX4 Wag  ┆ 6   │\n│ Hornet 4 Drive ┆ 6   │\n│ Valiant        ┆ 6   │\n│ Merc 280       ┆ 6   │\n│ Merc 280C      ┆ 6   │\n│ Ferrari Dino   ┆ 6   │\n└────────────────┴─────┘\n\nprint(carsr.collect())\n\nshape: (11, 2)\n┌────────────────┬─────┐\n│ car            ┆ hp  │\n│ ---            ┆ --- │\n│ str            ┆ i64 │\n╞════════════════╪═════╡\n│ Datsun 710     ┆ 93  │\n│ Valiant        ┆ 105 │\n│ Merc 240D      ┆ 62  │\n│ Merc 230       ┆ 95  │\n│ Fiat 128       ┆ 66  │\n│ …              ┆ …   │\n│ Toyota Corolla ┆ 65  │\n│ Toyota Corona  ┆ 97  │\n│ Fiat X1-9      ┆ 66  │\n│ Porsche 914-2  ┆ 91  │\n│ Volvo 142E     ┆ 109 │\n└────────────────┴─────┘\n\nprint(carsl.join(carsr,on=\"car\",how=\"left\").collect())\n\nshape: (7, 3)\n┌────────────────┬─────┬──────┐\n│ car            ┆ cyl ┆ hp   │\n│ ---            ┆ --- ┆ ---  │\n│ str            ┆ i64 ┆ i64  │\n╞════════════════╪═════╪══════╡\n│ Mazda RX4      ┆ 6   ┆ null │\n│ Mazda RX4 Wag  ┆ 6   ┆ null │\n│ Hornet 4 Drive ┆ 6   ┆ null │\n│ Valiant        ┆ 6   ┆ 105  │\n│ Merc 280       ┆ 6   ┆ null │\n│ Merc 280C      ┆ 6   ┆ null │\n│ Ferrari Dino   ┆ 6   ┆ null │\n└────────────────┴─────┴──────┘\n\nprint(carsl.join(carsr,on=\"car\",how=\"right\").collect())\n\nshape: (11, 3)\n┌──────┬────────────────┬─────┐\n│ cyl  ┆ car            ┆ hp  │\n│ ---  ┆ ---            ┆ --- │\n│ i64  ┆ str            ┆ i64 │\n╞══════╪════════════════╪═════╡\n│ null ┆ Datsun 710     ┆ 93  │\n│ 6    ┆ Valiant        ┆ 105 │\n│ null ┆ Merc 240D      ┆ 62  │\n│ null ┆ Merc 230       ┆ 95  │\n│ null ┆ Fiat 128       ┆ 66  │\n│ …    ┆ …              ┆ …   │\n│ null ┆ Toyota Corolla ┆ 65  │\n│ null ┆ Toyota Corona  ┆ 97  │\n│ null ┆ Fiat X1-9      ┆ 66  │\n│ null ┆ Porsche 914-2  ┆ 91  │\n│ null ┆ Volvo 142E     ┆ 109 │\n└──────┴────────────────┴─────┘\n\nprint(carsl.join(carsr,on=\"car\",how=\"inner\").collect())\n\nshape: (1, 3)\n┌─────────┬─────┬─────┐\n│ car     ┆ cyl ┆ hp  │\n│ ---     ┆ --- ┆ --- │\n│ str     ┆ i64 ┆ i64 │\n╞═════════╪═════╪═════╡\n│ Valiant ┆ 6   ┆ 105 │\n└─────────┴─────┴─────┘\n\nprint(carsl.join(carsr,on=\"car\",how=\"full\",coalesce=True).collect())\n\nshape: (17, 3)\n┌────────────────┬──────┬──────┐\n│ car            ┆ cyl  ┆ hp   │\n│ ---            ┆ ---  ┆ ---  │\n│ str            ┆ i64  ┆ i64  │\n╞════════════════╪══════╪══════╡\n│ Datsun 710     ┆ null ┆ 93   │\n│ Valiant        ┆ 6    ┆ 105  │\n│ Merc 240D      ┆ null ┆ 62   │\n│ Merc 230       ┆ null ┆ 95   │\n│ Fiat 128       ┆ null ┆ 66   │\n│ …              ┆ …    ┆ …    │\n│ Merc 280       ┆ 6    ┆ null │\n│ Mazda RX4      ┆ 6    ┆ null │\n│ Ferrari Dino   ┆ 6    ┆ null │\n│ Hornet 4 Drive ┆ 6    ┆ null │\n│ Merc 280C      ┆ 6    ┆ null │\n└────────────────┴──────┴──────┘\n\nprint(carsl.join(carsr,on=\"car\",how=\"anti\").collect())\n\nshape: (6, 2)\n┌────────────────┬─────┐\n│ car            ┆ cyl │\n│ ---            ┆ --- │\n│ str            ┆ i64 │\n╞════════════════╪═════╡\n│ Mazda RX4      ┆ 6   │\n│ Mazda RX4 Wag  ┆ 6   │\n│ Hornet 4 Drive ┆ 6   │\n│ Merc 280       ┆ 6   │\n│ Merc 280C      ┆ 6   │\n│ Ferrari Dino   ┆ 6   │\n└────────────────┴─────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-7.-function-for-n-pct-by-grouping-variables",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-7.-function-for-n-pct-by-grouping-variables",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 7. Function for n & pct by Grouping Variables",
    "text": "Example 7. Function for n & pct by Grouping Variables\nHere we want a custom function to create simple, list-style frequency tables based on one or more variables provided by the user.\n\nR Version\nWe use dynamic dots (...) here to tunnel in the variables provided by the user in the function call. This is powerful and flexible, allowing for 0+ variables provided as naked symbols rather than strings (doit()); an alternative version (doit2()) also uses dynamic dots, but with the intention to call with variable names provided as strings—this scales up better and is more comparable to the python version.\n\nlibrary(rlang)\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\nlibrary(purrr)\n\ndoit&lt;-function(df,...){\n  df %&gt;%\n    ungroup() %&gt;%\n    mutate(N=n()) %&gt;%\n    group_by(...) %&gt;%\n    summarize(n=n(),pct=n()*100/mean(N),.groups=\"drop\") %&gt;%\n    mutate(cumn=cumsum(n),cumpct=cumsum(pct))\n}\n\ndoit(mtcars)\n\n# A tibble: 1 × 4\n      n   pct  cumn cumpct\n  &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1    32   100    32    100\n\ndoit(mtcars,cyl)\n\n# A tibble: 3 × 5\n    cyl     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4    11  34.4    11   34.4\n2     6     7  21.9    18   56.2\n3     8    14  43.8    32  100  \n\ndoit(mtcars,cyl,gear)\n\n# A tibble: 8 × 6\n    cyl  gear     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     3     1  3.12     1   3.12\n2     4     4     8 25        9  28.1 \n3     4     5     2  6.25    11  34.4 \n4     6     3     2  6.25    13  40.6 \n5     6     4     4 12.5     17  53.1 \n6     6     5     1  3.12    18  56.2 \n7     8     3    12 37.5     30  93.8 \n8     8     5     2  6.25    32 100   \n\ndoit2&lt;-function(df,...){\n    vars&lt;-dots_list(...) %&gt;%\n        list_c() %&gt;%\n        syms()\n\n    df %&gt;%\n        ungroup() %&gt;%\n        mutate(N=n()) %&gt;%\n        group_by(!!!vars) %&gt;%\n        summarize(n=n(),pct=n()*100/mean(N),.groups=\"drop\") %&gt;%\n        mutate(cumn=cumsum(n),cumpct=cumsum(pct))\n}\n\ndoit2(mtcars)\n\n# A tibble: 1 × 4\n      n   pct  cumn cumpct\n  &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1    32   100    32    100\n\ndoit2(mtcars,\"cyl\")\n\n# A tibble: 3 × 5\n    cyl     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4    11  34.4    11   34.4\n2     6     7  21.9    18   56.2\n3     8    14  43.8    32  100  \n\ndoit2(mtcars,\"cyl\",\"gear\")\n\n# A tibble: 8 × 6\n    cyl  gear     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     3     1  3.12     1   3.12\n2     4     4     8 25        9  28.1 \n3     4     5     2  6.25    11  34.4 \n4     6     3     2  6.25    13  40.6 \n5     6     4     4 12.5     17  53.1 \n6     6     5     1  3.12    18  56.2 \n7     8     3    12 37.5     30  93.8 \n8     8     5     2  6.25    32 100   \n\n\n\n\nPython Version\nThe version below gets very close! The only differences are that the python version of doit() doesn’t work as-is if 0 variables are provided, and the variable names are passed as strings (i.e., this isn’t optional as with the tidy versions). This latter point should actually simplify some situations that are complex due to data mask ambiguities in tidy functions.\n\ndef doit(df,*argv):\n    q=(\n        df\n        .with_columns(pl.len().alias(\"N\"))\n        .group_by(*argv)\n        .agg(\n            pl.len().alias(\"n\")\n            ,((pl.len()*100)/pl.col(\"N\").mean()).alias(\"pct\")\n        )\n        .sort(*argv)\n        .with_columns(\n            pl.col(\"n\").cum_sum().alias(\"cumn\")\n            ,pl.col(\"pct\").cum_sum().alias(\"cumpct\")\n        )\n    )\n    table=q.collect()\n    print(table)\n\ndoit(mtcars,\"cyl\")\n\nshape: (3, 5)\n┌─────┬─────┬────────┬──────┬────────┐\n│ cyl ┆ n   ┆ pct    ┆ cumn ┆ cumpct │\n│ --- ┆ --- ┆ ---    ┆ ---  ┆ ---    │\n│ f64 ┆ u32 ┆ f64    ┆ u32  ┆ f64    │\n╞═════╪═════╪════════╪══════╪════════╡\n│ 4.0 ┆ 11  ┆ 34.375 ┆ 11   ┆ 34.375 │\n│ 6.0 ┆ 7   ┆ 21.875 ┆ 18   ┆ 56.25  │\n│ 8.0 ┆ 14  ┆ 43.75  ┆ 32   ┆ 100.0  │\n└─────┴─────┴────────┴──────┴────────┘\n\ndoit(mtcars,\"cyl\",\"gear\")\n\nshape: (8, 6)\n┌─────┬──────┬─────┬───────┬──────┬────────┐\n│ cyl ┆ gear ┆ n   ┆ pct   ┆ cumn ┆ cumpct │\n│ --- ┆ ---  ┆ --- ┆ ---   ┆ ---  ┆ ---    │\n│ f64 ┆ f64  ┆ u32 ┆ f64   ┆ u32  ┆ f64    │\n╞═════╪══════╪═════╪═══════╪══════╪════════╡\n│ 4.0 ┆ 3.0  ┆ 1   ┆ 3.125 ┆ 1    ┆ 3.125  │\n│ 4.0 ┆ 4.0  ┆ 8   ┆ 25.0  ┆ 9    ┆ 28.125 │\n│ 4.0 ┆ 5.0  ┆ 2   ┆ 6.25  ┆ 11   ┆ 34.375 │\n│ 6.0 ┆ 3.0  ┆ 2   ┆ 6.25  ┆ 13   ┆ 40.625 │\n│ 6.0 ┆ 4.0  ┆ 4   ┆ 12.5  ┆ 17   ┆ 53.125 │\n│ 6.0 ┆ 5.0  ┆ 1   ┆ 3.125 ┆ 18   ┆ 56.25  │\n│ 8.0 ┆ 3.0  ┆ 12  ┆ 37.5  ┆ 30   ┆ 93.75  │\n│ 8.0 ┆ 5.0  ┆ 2   ┆ 6.25  ┆ 32   ┆ 100.0  │\n└─────┴──────┴─────┴───────┴──────┴────────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-8.-iterate-a-custom-function",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-8.-iterate-a-custom-function",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 8. Iterate a Custom Function",
    "text": "Example 8. Iterate a Custom Function\nHere we want to apply the doit functions over parameters.\n\nR Version\nWe can use purrr::pmap() in the R version with a list of parameters. Since we defined the R version of doit to take naked symbols, the mapped version is kind of clunky—a cleaner alternative based on doit2 highlights this point.\n\nparms&lt;-list(\n    list(mtcars,mtcars)\n    ,\"var1\"=list(mtcars$cyl,mtcars$cyl)\n    ,\"var2\"=list(mtcars$gear,mtcars$am)\n)\n\npmap(parms,doit)\n\n[[1]]\n# A tibble: 8 × 6\n   var1  var2     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     3     1  3.12     1   3.12\n2     4     4     8 25        9  28.1 \n3     4     5     2  6.25    11  34.4 \n4     6     3     2  6.25    13  40.6 \n5     6     4     4 12.5     17  53.1 \n6     6     5     1  3.12    18  56.2 \n7     8     3    12 37.5     30  93.8 \n8     8     5     2  6.25    32 100   \n\n[[2]]\n# A tibble: 6 × 6\n   var1  var2     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     0     3  9.38     3   9.38\n2     4     1     8 25       11  34.4 \n3     6     0     4 12.5     15  46.9 \n4     6     1     3  9.38    18  56.2 \n5     8     0    12 37.5     30  93.8 \n6     8     1     2  6.25    32 100   \n\nparms2&lt;-list(\n    list(mtcars,mtcars)\n    ,c(\"cyl\",\"cyl\")\n    ,c(\"gear\",\"am\")\n)\n\npmap(parms2,doit2)\n\n[[1]]\n# A tibble: 8 × 6\n    cyl  gear     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     3     1  3.12     1   3.12\n2     4     4     8 25        9  28.1 \n3     4     5     2  6.25    11  34.4 \n4     6     3     2  6.25    13  40.6 \n5     6     4     4 12.5     17  53.1 \n6     6     5     1  3.12    18  56.2 \n7     8     3    12 37.5     30  93.8 \n8     8     5     2  6.25    32 100   \n\n[[2]]\n# A tibble: 6 × 6\n    cyl    am     n   pct  cumn cumpct\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     4     0     3  9.38     3   9.38\n2     4     1     8 25       11  34.4 \n3     6     0     4 12.5     15  46.9 \n4     6     1     3  9.38    18  56.2 \n5     8     0    12 37.5     30  93.8 \n6     8     1     2  6.25    32 100   \n\n\n\n\nPython Version\nHere we combine 3 parameter lists into a single iterator object via zip—we can then map doit over parms via itertools.starmap.\n\nimport itertools as it\n\nparms=zip(\n    [mtcars,mtcars]\n    ,['cyl','cyl']\n    ,['gear','am']\n)\n\nlist(it.starmap(doit,parms))\n\nshape: (8, 6)\n┌─────┬──────┬─────┬───────┬──────┬────────┐\n│ cyl ┆ gear ┆ n   ┆ pct   ┆ cumn ┆ cumpct │\n│ --- ┆ ---  ┆ --- ┆ ---   ┆ ---  ┆ ---    │\n│ f64 ┆ f64  ┆ u32 ┆ f64   ┆ u32  ┆ f64    │\n╞═════╪══════╪═════╪═══════╪══════╪════════╡\n│ 4.0 ┆ 3.0  ┆ 1   ┆ 3.125 ┆ 1    ┆ 3.125  │\n│ 4.0 ┆ 4.0  ┆ 8   ┆ 25.0  ┆ 9    ┆ 28.125 │\n│ 4.0 ┆ 5.0  ┆ 2   ┆ 6.25  ┆ 11   ┆ 34.375 │\n│ 6.0 ┆ 3.0  ┆ 2   ┆ 6.25  ┆ 13   ┆ 40.625 │\n│ 6.0 ┆ 4.0  ┆ 4   ┆ 12.5  ┆ 17   ┆ 53.125 │\n│ 6.0 ┆ 5.0  ┆ 1   ┆ 3.125 ┆ 18   ┆ 56.25  │\n│ 8.0 ┆ 3.0  ┆ 12  ┆ 37.5  ┆ 30   ┆ 93.75  │\n│ 8.0 ┆ 5.0  ┆ 2   ┆ 6.25  ┆ 32   ┆ 100.0  │\n└─────┴──────┴─────┴───────┴──────┴────────┘\nshape: (6, 6)\n┌─────┬─────┬─────┬───────┬──────┬────────┐\n│ cyl ┆ am  ┆ n   ┆ pct   ┆ cumn ┆ cumpct │\n│ --- ┆ --- ┆ --- ┆ ---   ┆ ---  ┆ ---    │\n│ f64 ┆ f64 ┆ u32 ┆ f64   ┆ u32  ┆ f64    │\n╞═════╪═════╪═════╪═══════╪══════╪════════╡\n│ 4.0 ┆ 0.0 ┆ 3   ┆ 9.375 ┆ 3    ┆ 9.375  │\n│ 4.0 ┆ 1.0 ┆ 8   ┆ 25.0  ┆ 11   ┆ 34.375 │\n│ 6.0 ┆ 0.0 ┆ 4   ┆ 12.5  ┆ 15   ┆ 46.875 │\n│ 6.0 ┆ 1.0 ┆ 3   ┆ 9.375 ┆ 18   ┆ 56.25  │\n│ 8.0 ┆ 0.0 ┆ 12  ┆ 37.5  ┆ 30   ┆ 93.75  │\n│ 8.0 ┆ 1.0 ┆ 2   ┆ 6.25  ┆ 32   ┆ 100.0  │\n└─────┴─────┴─────┴───────┴──────┴────────┘\n[None, None]"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-9.-stack-data-frames-by-list-binding-with-map-and-anonymous-function",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-9.-stack-data-frames-by-list-binding-with-map-and-anonymous-function",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 9. Stack Data Frames by List Binding with Map and Anonymous Function",
    "text": "Example 9. Stack Data Frames by List Binding with Map and Anonymous Function\nWhat we’re achieving with this example—returning mtcars—isn’t very useful, but it illustrates a common task: mapping an anonymous function over a vector to create a list of data frames which are subsequently stacked together via row binding. In other words, in this example we’re reassembling mtcars by stacking together portions returned from each iteration of map.\n\nR Version\nPretty straight forward. Note that parms is a vector here.\n\nparms&lt;-distinct(mtcars,cyl) %&gt;%\n  pull()\n\nlist&lt;-map(\n  parms\n  ,function (x){\n    mtcars %&gt;%\n      dplyr::filter(cyl==x) %&gt;%\n      arrange(desc(mpg))\n  }\n)\n\ndf&lt;-list_rbind(list)\n\nprint(df)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n\n\n\nPython Version\nThis is extremely similar. Note that we’re pulling from the csv of mtcars to utilize eager evaluation (for simplicity).\n\nmtcars=pl.read_csv(\"mtcars.csv\")\n\niterator=mtcars.get_column(\"cyl\").unique()\n\ndf=map(\n    lambda x: (\n        mtcars\n        .filter(pl.col(\"cyl\")==x)\n        .sort(\"mpg\",descending=True)\n    )\n    ,iterator\n)\n\ndf=pl.concat(list(df))\n\nprint(df)\n\nshape: (32, 11)\n┌──────┬─────┬───────┬─────┬───┬─────┬─────┬──────┬──────┐\n│ mpg  ┆ cyl ┆ disp  ┆ hp  ┆ … ┆ vs  ┆ am  ┆ gear ┆ carb │\n│ ---  ┆ --- ┆ ---   ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ ---  │\n│ f64  ┆ i64 ┆ f64   ┆ i64 ┆   ┆ i64 ┆ i64 ┆ i64  ┆ i64  │\n╞══════╪═════╪═══════╪═════╪═══╪═════╪═════╪══════╪══════╡\n│ 33.9 ┆ 4   ┆ 71.1  ┆ 65  ┆ … ┆ 1   ┆ 1   ┆ 4    ┆ 1    │\n│ 32.4 ┆ 4   ┆ 78.7  ┆ 66  ┆ … ┆ 1   ┆ 1   ┆ 4    ┆ 1    │\n│ 30.4 ┆ 4   ┆ 75.7  ┆ 52  ┆ … ┆ 1   ┆ 1   ┆ 4    ┆ 2    │\n│ 30.4 ┆ 4   ┆ 95.1  ┆ 113 ┆ … ┆ 1   ┆ 1   ┆ 5    ┆ 2    │\n│ 27.3 ┆ 4   ┆ 79.0  ┆ 66  ┆ … ┆ 1   ┆ 1   ┆ 4    ┆ 1    │\n│ …    ┆ …   ┆ …     ┆ …   ┆ … ┆ …   ┆ …   ┆ …    ┆ …    │\n│ 14.7 ┆ 8   ┆ 440.0 ┆ 230 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n│ 14.3 ┆ 8   ┆ 360.0 ┆ 245 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n│ 13.3 ┆ 8   ┆ 350.0 ┆ 245 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n│ 10.4 ┆ 8   ┆ 472.0 ┆ 205 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n│ 10.4 ┆ 8   ┆ 460.0 ┆ 215 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n└──────┴─────┴───────┴─────┴───┴─────┴─────┴──────┴──────┘"
  },
  {
    "objectID": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-10.-stack-data-frames-by-list-binding-with-pmap-and-anonymous-function-of-dots-...",
    "href": "posts/2025-03-19-TidyR-to-PolarsPython/index.html#example-10.-stack-data-frames-by-list-binding-with-pmap-and-anonymous-function-of-dots-...",
    "title": "Recreating Some Tidy-Style R Operations with Python and Polars",
    "section": "Example 10. Stack Data Frames by List Binding with pmap and Anonymous Function of Dots (...)",
    "text": "Example 10. Stack Data Frames by List Binding with pmap and Anonymous Function of Dots (...)\nThis example generalizes the previous one to use a data frame with any number of columns (here we’re just using 2) to parameterize the mapping.\n\nR Version\nDynamic dots are captured in the list parms within the function and column values are referenced as elements of that list.\n\nparms&lt;-distinct(mtcars,cyl,gear)\n\nlist&lt;-pmap(\n  parms\n  ,function (...){\n    parms&lt;-rlang::dots_list(...)\n    mtcars %&gt;%\n      dplyr::filter(cyl==parms$cyl & gear==parms$gear) \n  }\n)\n\ndf&lt;-list_rbind(list)\n\nprint(df)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\n\nPython Version\nThis is very similar to the R version above. The key is that each row of the data frame parms is turned into a dictionary (i.e., has the form {'key1':_key1val_,...,'keyk':_keykval_}). iterator is then a list of dictionaries (iter_rows() returns dictionaries when named=True) which allows us to capture a single row of all parameters needed in the body of the anonymous function with the single parameter dctnry. The parameter values can be referenced by the original parms data frame variable name in the function body via the dictionary method get(). Note that there is a Polars DataFrame method map_rows() that does something similar, but there’s no way to preserve variable names for reference inside the function, so this approach seems preferrable.\n\nmtcars=pl.read_csv(\"mtcars.csv\")\n\nparms=(\n    mtcars\n    .group_by(\"cyl\",\"gear\")\n    .agg()\n)\n\niterator=list(parms.iter_rows(named=True))\n\ndfs=map(\n    lambda dctnry: (\n        mtcars\n        .filter(\n            (pl.col(\"cyl\")==dctnry.get('cyl')) & \n            (pl.col(\"gear\")==dctnry.get('gear'))\n        )\n    )\n    ,iterator\n)\n\ndf=pl.concat(list(dfs))\n\nprint(df)\n\nshape: (32, 11)\n┌──────┬─────┬───────┬─────┬───┬─────┬─────┬──────┬──────┐\n│ mpg  ┆ cyl ┆ disp  ┆ hp  ┆ … ┆ vs  ┆ am  ┆ gear ┆ carb │\n│ ---  ┆ --- ┆ ---   ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ ---  │\n│ f64  ┆ i64 ┆ f64   ┆ i64 ┆   ┆ i64 ┆ i64 ┆ i64  ┆ i64  │\n╞══════╪═════╪═══════╪═════╪═══╪═════╪═════╪══════╪══════╡\n│ 18.7 ┆ 8   ┆ 360.0 ┆ 175 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 2    │\n│ 14.3 ┆ 8   ┆ 360.0 ┆ 245 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 4    │\n│ 16.4 ┆ 8   ┆ 275.8 ┆ 180 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 3    │\n│ 17.3 ┆ 8   ┆ 275.8 ┆ 180 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 3    │\n│ 15.2 ┆ 8   ┆ 275.8 ┆ 180 ┆ … ┆ 0   ┆ 0   ┆ 3    ┆ 3    │\n│ …    ┆ …   ┆ …     ┆ …   ┆ … ┆ …   ┆ …   ┆ …    ┆ …    │\n│ 21.4 ┆ 6   ┆ 258.0 ┆ 110 ┆ … ┆ 1   ┆ 0   ┆ 3    ┆ 1    │\n│ 18.1 ┆ 6   ┆ 225.0 ┆ 105 ┆ … ┆ 1   ┆ 0   ┆ 3    ┆ 1    │\n│ 15.8 ┆ 8   ┆ 351.0 ┆ 264 ┆ … ┆ 0   ┆ 1   ┆ 5    ┆ 4    │\n│ 15.0 ┆ 8   ┆ 301.0 ┆ 335 ┆ … ┆ 0   ┆ 1   ┆ 5    ┆ 8    │\n│ 21.5 ┆ 4   ┆ 120.1 ┆ 97  ┆ … ┆ 1   ┆ 0   ┆ 3    ┆ 1    │\n└──────┴─────┴───────┴─────┴───┴─────┴─────┴──────┴──────┘"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]