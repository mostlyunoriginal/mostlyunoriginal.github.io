{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Thread Pooling a Python Process to Build a Stacked ACS DataFrame\"\n",
        "description: \"This post demonstrates how to take a standard list comprehension of calls to the Census API and pool it for dramatic time savings.\"\n",
        "date: 2025-08-05\n",
        "author:\n",
        "    - name: Lance Couzens\n",
        "      url: https://mostlyunoriginal.github.io  \n",
        "categories: [Python, Thread Pooling]\n",
        "citation: \n",
        "  url: https://mostlyunoriginal.github.io/posts/2025-08-05-Python-Thread-Pooling/\n",
        "image: Preview.png\n",
        "draft: false\n",
        "lightbox: true\n",
        "---\n",
        "\n",
        "\n",
        "## Background\n",
        "\n",
        "As part of a machine learning class I've been building for a client, I've provided the option to join county-level ACS estimates onto the training and prediction data. County data require state-level calls to the Census API, and including all states and a few years, sequential pulls were proving to be a significant bottleneck. Swapping the sequential process out in favor of a pooled one yielded dramatic time savings. This post demonstrates that change via an abstracted, simplified example.\n",
        "\n",
        "## Data Ingestion\n",
        "\n",
        "The `get_acs` function pulls county-level 5-year ACS estimates from the Census API for a single state and year. We also do all the importing and environment variable creation we'll need for both versions of subsequent steps (we want those to be as close to apples-to-apples as possible since we'll be comparing timing).\n"
      ],
      "id": "43f9c54e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import polars as pl\n",
        "import os\n",
        "import itertools as it\n",
        "import us\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# states and years to pull ACS 5-year county-level estimates for\n",
        "states = [state.fips for state in us.states.STATES]\n",
        "years = [2020, 2021]\n",
        "\n",
        "# list of tuples representing cartesian product of individual state and year lists\n",
        "state_year_pairs = list(it.product(states, years))\n",
        "\n",
        "def get_acs(\n",
        "    state_fips: str, \n",
        "    year: int,\n",
        "    api_key: str = None, \n",
        "    acs_vars: dict = {\n",
        "            \"B01001_001E\": \"pop_total\",\n",
        "            \"B01002_001E\": \"median_age\",\n",
        "            \"B19013_001E\": \"med_hh_income\",\n",
        "            \"B23025_005E\": \"unemployed\",\n",
        "            \"B25077_001E\": \"med_home_value\",\n",
        "            \"B25064_001E\": \"med_rent\",\n",
        "        }\n",
        "    ):\n",
        "\n",
        "    url = (\n",
        "        f\"https://api.census.gov/data/{year}/acs/\"\n",
        "        f\"acs5?get={','.join(acs_vars.keys())}\"\n",
        "        f\"&for=county:*&in=state:{state_fips}\"\n",
        "        f\"&key={api_key}\"\n",
        "    )\n",
        "\n",
        "    # issue request and parse resulting json\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if len(data)>1:\n",
        "            acs_df = pl.DataFrame(\n",
        "                data[1:], \n",
        "                schema=data[0],\n",
        "                orient=\"row\",\n",
        "            ).rename(acs_vars).with_columns(year=pl.lit(year))\n",
        "            return acs_df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return None"
      ],
      "id": "7aaaa761",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential Implementation via List Comprehension\n",
        "\n",
        "Here we grab and stack the data for all states and two data years via list comprehension.\n"
      ],
      "id": "8fbcb024"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "\n",
        "all_acs = [\n",
        "    get_acs(state_fips, year, api_key=os.getenv(\"CENSUS_API_KEY\"))\n",
        "    for state_fips, year in state_year_pairs\n",
        "]\n",
        "\n",
        "# concatenate all results\n",
        "if all_acs:\n",
        "    stacked_acs = pl.concat(all_acs)\n",
        "    print(f\"Data stacked and {stacked_acs.height} records returned\")\n",
        "else:\n",
        "    print(\"No data was fetched.\")"
      ],
      "id": "116da9a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Threaded Alternative\n",
        "\n",
        "Here we leverage the `concurrent.futures` module from the standard Python library to issue all API calls simultaneously, gathering results asynchronously as they become available.\n"
      ],
      "id": "e637b5ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "\n",
        "all_acs = []\n",
        "with ThreadPoolExecutor(max_workers=len(state_year_pairs)) as executor:\n",
        "    futures = {\n",
        "        executor.submit(\n",
        "          get_acs, \n",
        "          state_fips, \n",
        "          year, \n",
        "          api_key=os.getenv(\"CENSUS_API_KEY\")\n",
        "        ): (state_fips, year)\n",
        "        for state_fips, year in state_year_pairs\n",
        "    }\n",
        "    for future in as_completed(futures.keys()):\n",
        "        pair = futures[future]\n",
        "        try:\n",
        "          result = future.result()\n",
        "          if result is not None:\n",
        "              all_acs.append(result)\n",
        "        except Exception as e:\n",
        "          print(f\"ERROR: Something went wrong for state={pair[0]}, year={pair[1]}: {e}\")\n",
        "\n",
        "# concatenate all results\n",
        "if all_acs:\n",
        "    stacked_acs = pl.concat(all_acs)\n",
        "    print(f\"Data stacked and {stacked_acs.height} records returned\")\n",
        "else:\n",
        "    print(\"No data was fetched.\")            "
      ],
      "id": "88b99b9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this alternative, `ThreadPoolExecutor` is used as a context manager to issue `get_acs` calls and \"future states\" are gathered via a dictionary comprehension. Note, however, that this dictionary flips the script a bit with the returned data frames in the key position and the state, year tuples they correspond to in the values position. This is possible because the returned data frames are unique / hashable, and it allows us to report out for which state and year a failure occurs.\n",
        "\n",
        "As results are returned, they are assessed and appended to the `all_acs` list for subsequent concatenation. As shown by the timing stats under each code chunk, we see a significant speed up from over 1 minute to just a handful of seconds. Note also that we specify a `max_workers` count equivalent to the number of times we will be hitting the API. For thread pooling, we're not limited to our machine's core count, as we're not running a truly parallelized process that utilizes multiple cores simultaneously."
      ],
      "id": "7e345114"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\gcouzens\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}