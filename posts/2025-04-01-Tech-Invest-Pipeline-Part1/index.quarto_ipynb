{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Building a Technical Trading Data & Analytics Pipeline\"\n",
        "description: \"This post is the first in a series chronicling a personal project: setting up a technical investment data and analytics pipeline with Python and R\"\n",
        "date: 2025-04-01\n",
        "author:\n",
        "    - name: Lance Couzens\n",
        "      url: https://mostlyunoriginal.github.io  \n",
        "categories: [R, Python, Investing]\n",
        "citation: \n",
        "  url: https://mostlyunoriginal.github.io/posts/2025-04-01-Tech-Invest-Pipeline-Part1/\n",
        "image: PReview.webp\n",
        "draft: true\n",
        "---\n",
        "\n",
        "\n",
        "## Background\n",
        "\n",
        "As a person with a strong background in analytics and a love of programming, I've always wanted to have a go at technical investing, using my own pipeline. I've finally taken the project on in earnest, and I'm going to chronicle the twists and turns it takes on the blog---this is Part 1.\n",
        "\n",
        "So, what do I mean by 'pipeline' in this context? Essentially, I mean ingesting market data, algorithmically curating buy candidates, tracking existing positions for sell signals, and all the nitty gritty in-betweens that entails. I envision four high-level components:\n",
        "\n",
        "1.  Data ingestion and transformation,\n",
        "\n",
        "2.  Application of a model to identify and rank buy candidates,\n",
        "\n",
        "3.  Daily, automated report creation to help me make decisions on buy candidates, and\n",
        "\n",
        "4.  Daily/intraday reporting/monitoring for existing positions.\n",
        "\n",
        "I've started on numbers 1 and 3, so I will cover some of that here.\n",
        "\n",
        "## Data Ingestion\n",
        "\n",
        "I'm going to focus exclusively on stocks to start, and I'll be getting my data from Polygon.io---they have a variety of data offerings across various personal and business tiers (including a free option). I'll be using the Stocks Starter plan, which provides a decent amount of historical data aggregated in flat files by day or minute via Amazon S3 as well as near-real-time data via API.\n",
        "\n",
        "My core data object that will serve as input to the curation model will be a Python (Polars) DataFrame of daily aggregates for all U.S. stocks (\\~10K) over a flexible window of time through the prior trading day. I'll build the DataFrame from flat files using the Python Boto3 SDK for S3 and two custom functions.\n",
        "\n",
        "### Function 1: List Files\n",
        "\n",
        "This function returns a list of file names satisfying parameterized criteria (day vs. minute, last day, window size, etc.).\n"
      ],
      "id": "86955b8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def list_hist_files(\n",
        "        kind='day_aggs',\n",
        "        last_day='2025-03-28',\n",
        "        window=30,\n",
        "        prefix='us_stocks_sip',\n",
        "        bucket_name='flatfiles',\n",
        "        bookend=False,\n",
        "    ):\n",
        "\n",
        "    session=boto3.Session(\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "    )\n",
        "\n",
        "    s3=session.client(\n",
        "        's3',\n",
        "        endpoint_url='https://files.polygon.io',\n",
        "        config=Config(signature_version='s3v4'),\n",
        "    )\n",
        "\n",
        "    paginator=s3.get_paginator('list_objects_v2')\n",
        "\n",
        "    dates=[]\n",
        "    end_date=datetime.strptime(last_day,'%Y-%m-%d')\n",
        "    for delta in range(window+1):\n",
        "        temp_past_date=end_date-timedelta(days=delta)\n",
        "        dates.append(datetime.strftime(temp_past_date,'%Y-%m-%d'))\n",
        "\n",
        "    files=[]\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
        "        for obj in page['Contents']:\n",
        "            if obj['Key'].find(kind)>=0 and re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',obj['Key']) in dates: \n",
        "                files.append(obj['Key'])\n",
        "\n",
        "    if bookend and len(files)>2:\n",
        "        files=[files[0],files[-1]]\n",
        "\n",
        "    return files"
      ],
      "id": "79cc4dd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 2: Ingest Files\n",
        "\n",
        "The second function reads a single, dated file for the full market or for an optional subset of tickers into memory and returns a Polars DataFrame. This function has a simple positional parameterization, as it's intended to be called via the `itertools.starmap()` functional.\n"
      ],
      "id": "ee70b076"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_hist_data(file,tickers):\n",
        "\n",
        "    date=re.sub('.*(\\\\d{4}-\\\\d{2}-\\\\d{2}).*','\\\\1',file)\n",
        "\n",
        "    session=boto3.Session(\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "    )\n",
        "\n",
        "    s3=session.client(\n",
        "        's3',\n",
        "        endpoint_url='https://files.polygon.io',\n",
        "        config=Config(signature_version='s3v4'),\n",
        "    )\n",
        "\n",
        "    response=s3.get_object(Bucket='flatfiles',Key=file)\n",
        "    compressed_data=response[\"Body\"].read()\n",
        "\n",
        "    with gzip.GzipFile(fileobj=io.BytesIO(compressed_data),mode=\"rb\") as f:\n",
        "        if tickers: df=pl.scan_csv(f).filter(pl.col('ticker').is_in(tickers)).collect()\n",
        "        else: df=pl.read_csv(f)\n",
        "\n",
        "    return df.insert_column(1,pl.lit(date).alias(\"date\"))"
      ],
      "id": "56c1ab78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transormation\n",
        "\n",
        "Next, I pull in the data and create some metrics, including short and long simple moving averages, moving average convergence/divergence (MACD) indicator and signal (its moving average), and the relative strength index (RSI).\n"
      ],
      "id": "b31c442e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import itertools as it\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "import tempfile\n",
        "import gzip\n",
        "import io\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from polygon import RESTClient\n",
        "import polars.selectors as cs\n",
        "from dataclasses import asdict\n",
        "\n",
        "with open(\"/Users/lance/Desktop/TechInvest/scripts/sandbox/01_GetHistAggs.py\") as script:\n",
        "    exec(script.read())\n",
        "\n",
        "with open(\"/Users/lance/Desktop/TechInvest/keys.py\") as script:\n",
        "    exec(script.read())\n",
        "\n",
        "iterator=it.product(\n",
        "    list_hist_files(kind=\"day_aggs\",last_day=r.params[\"ref_date\"],window=r.params[\"window\"],bookend=False),\n",
        "    [[r.params[\"ticker\"]]],\n",
        ")\n",
        "\n",
        "df=it.starmap(get_hist_data,iterator)\n",
        "\n",
        "df=(\n",
        "    pl.concat(list(df))\n",
        "    .lazy()\n",
        "    .sort(\"ticker\",\"window_start\")\n",
        "    .with_columns(\n",
        "        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_l\"]).over(\"ticker\").alias(\"sma_l\"),\n",
        "        pl.col(\"close\").rolling_mean(window_size=r.params[\"sma_s\"]).over(\"ticker\").alias(\"sma_s\"),\n",
        "        (pl.col(\"close\").ewm_mean(span=12,min_samples=12)-\n",
        "            pl.col(\"close\").ewm_mean(span=26,min_samples=26)\n",
        "        ).over(\"ticker\").alias(\"MACD\"),\n",
        "        (pl.col(\"close\")*2-pl.col(\"close\").rolling_sum(window_size=2)).over(\"ticker\").alias(\"rsi_diff\"),\n",
        "        pl.when(pl.col(\"close\")>pl.col(\"open\")).then(1)\n",
        "        .otherwise(-1)\n",
        "        .alias(\"candle_color\"),\n",
        "        pl.max_horizontal(\"open\",\"close\").alias(\"candle_high\"),\n",
        "        pl.min_horizontal(\"open\",\"close\").alias(\"candle_low\"),\n",
        "        pl.mean_horizontal(\"open\",\"close\").alias(\"candle_mid\"),\n",
        "    )\n",
        "    .with_columns(\n",
        "        pl.col(\"MACD\").ewm_mean(span=9,min_samples=9).over(\"ticker\").alias(\"signal\"),\n",
        "        pl.when(pl.col(\"rsi_diff\")>0).then(\"rsi_diff\")\n",
        "        .otherwise(0)\n",
        "        .alias(\"U\"),\n",
        "        pl.when(pl.col(\"rsi_diff\")<0).then(-pl.col(\"rsi_diff\"))\n",
        "        .otherwise(0)\n",
        "        .alias(\"D\"),\n",
        "    )\n",
        "    .with_columns(\n",
        "        (pl.col(\"MACD\")-pl.col(\"signal\")).alias(\"histogram\"),\n",
        "        ((pl.col(\"U\").ewm_mean(min_samples=14,alpha=1/14))/(pl.col(\"D\").ewm_mean(min_samples=14,alpha=1/14))).alias(\"RS\"),\n",
        "    )\n",
        "    .with_columns((100-100/(1+pl.col(\"RS\"))).alias(\"RSI\"))\n",
        "    .filter(pl.col(\"signal\").is_not_null())\n",
        "    .collect()\n",
        "    .to_pandas()\n",
        ")"
      ],
      "id": "9abb1cd3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Library/Frameworks/Python.framework/Versions/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}