---
title: "Introducing cendat, a Python Library for Simplifying and Speeding up Use of the Census API"
description: "This post introduces cendat and demonstrates its use through examples."
date: 2025-08-13
author:
    - name: Lance Couzens
      url: https://mostlyunoriginal.github.io  
categories: [Python, cendat, Census, API]
citation: 
  url: https://mostlyunoriginal.github.io/posts/2025-08-13-Introducing-cendat/
image: Preview.png
draft: false
lightbox: true
---

## Background

The U.S. Census Bureau makes available through its free, public API mountains of data, though navigating it and acquiring the data you need can be tricky, especially for complex, nested geographic summary levels like those available in American Community Survey (ACS) data products. The Census Bureau provides buckets of features and nuance in its data portal <https://data.census.gov>, but this doesn't address the need to acquire data programmatically or in aggregate across the many 'drill-down' geographies required for certain summary levels. The API addresses the first problem, but on its own it does nothing to address the second. To acquire estimates at certain levels–even for a single state–may require several thousand API queries, and while data can be downloaded in bulk from <https://www2.census.gov>, you may have to sift through a lot of what you don't need to get at what you do. It's also not well-suited for obtaining data in-script.

`cendat` aims to address the programmatic need while leveraging automatic query building and concurrency to quickly and easily explore the API's data offerings, and pull in the data you need. There are other Census API wrapper libraries available for Python, and I honestly haven't assessed their capabilities more than superficially, but I like building my own tools, so here we are!

`cendat` can be installed with `pip`

```{bash}
pip install cendat
```

and you can also install `pandas` and/or `polars` at the same time to enable optional methods to work with acquired data

```{bash}
pip install cendat[pandas]
```

```{bash}
pip install cendat[polars]
```

```{bash}
pip install cendat[all]
```

You can check out its documentation at <https://pypi.org/project/cendat/>.

You need an API key to get the most out of `cendat`–you can get one here: <https://api.census.gov/data/key_signup.html>.

## Workflow

`cendat` includes two classes: `CenDatHelper` for exploring the API, locking in product, vintage, variable, and geographic selections, and getting data, and `CenDatResponse` to represent the returned data structure and provide methods for conversion to `pandas` or `polars` DataFrames.

These classes can be used if you already know exactly what you want and how to specify it, but they also streamline the process of figuring out and selecting what you want through a consistent list $\rightarrow$ set two-step that works for products, geographies, and variables. Once selections are locked in, the `get_data()` method builds the queries, issues them concurrently, and organizes the results into a digestible format.

## Step 1. Import and Instantiate

A `CenDatHelper` object can be instantiated without argument, or with the `key` parameter (as shown below) and/or `years`, which can be provided as an integer or list of integers. API key and years can also be provided later via the `load_key()` and `set_years()` methods, respectively. To start exploring, we'll provide a key but not specify any years of interest.

```{python}
from cendat import CenDatHelper
from dotenv import load_dotenv
import os
from pprint import pprint
import polars as pl
load_dotenv()

cdh = CenDatHelper(key=os.getenv('CENSUS_API_KEY'))
```

## Step 2. Explore Available Products

`cendat` (currently, as of ver. 0.2.2) supports all aggregate and microdata products available in the API (I'm open to adding timeseries product handling as well, if anyone requests it). To explore those products, we use the `list_products()` method. Let's start by looking for anything related to the ACS 5-year detailed tables.

```{python}
#| results: markup
potential_products = cdh.list_products(
    patterns=["acs", "5-year", "detailed"]
)

for product in potential_products:
    print(product['title'])
```

Okay, that gives us a good starting point. We'll get to filtering those results down in a moment, but first I want to highlight a couple of things about the code block above. First, you'll notice that I provided patterns to filter the products by via the `patterns` parameter. `patterns` takes a string or list of strings compiled into (case-insensitive) regular expressions. We can control whether the patterns all have to be met (the default) or only one or more through the `logic` parameter. To employ `any` logic set `logic=any`. If you don't speak regex, fear not--just supply substrings you'd like to match. We can also control which portion of the metadata the patterns will be used on. The default (which we've used here) is to operate on the titles. The alternative is to operate on the descriptions, which can be achieved by setting `match_in='desc'`. One other point. You'll notice that I selectively printed only `product['title']`. By default, the `list_products()` method returns a list of dictionaries of which the title is only one item (we can force it to return only the title by setting `to_dicts=False`). We'll see some more of what's returned after we filter our products down a bit more.

Okay, back to the results. A couple of things are obvious: the detailed tables products aren't named consistently across years, and there are some special population products that are being picked up as well. We can also see that there's a parenthetical portion to each product title that looks like a directory path--that's parsed from the JSON packet returned by the API and tacked on by `list_products()`. So, rather than trying to refine our patterns to capture the inconsistent portion of the titles, we can focus on that and try again.

```{python}
#| results: markup
potential_products = cdh.list_products(
    patterns=[r"acs/acs5\)"]
)

for product in potential_products:
    print(product['title'])
```

Now we've filtered down to the core product we want, but let's subset to only a couple years to keep things simple. We'll also take a look at the full contents of the products' dictionaries.

```{python}
#| results: markup
potential_products = cdh.list_products(
    patterns=[r"acs/acs5\)"],
    years=[2022, 2023]
)

for product in potential_products:
    pprint(product)
```

Okay, great--now we've got what we want, let's lock in our selection.

```{python}
#| results: markup
cdh.set_products()
```

## Step 3. Explore Variables
Now, let's start to think about which variables we want--according to the descriptions above, we have over 64,000 to choose from! Let's assume we're interested in median household income and average household size. We'll start by printing out the full dictionary for only the first variable, just to get a sense of the information available to us.

```{python}
#| results: markup
potential_variables = cdh.list_variables(
    patterns=["median household income", "household size"],
    logic=any
)

# look at the first to get a sense of what the full dictionaries have to offer
pprint(potential_variables[0])
```

When comparing the full dictionary of the first variable to just its label, we can see that there are nuances not captured in the label alone. When we print out the name and label for all found variables, we can see there is duplication in the label due to the lost nuance.

```{python}
#| results: markup

# print only the titles for all of the variables
for variable in potential_variables:
    print(variable['name'], variable['label'])
```

In our next pass we'll filter to promising variables based on label, but we'll print the variable name and concept, as that seems to capture the nuance we need to be aware of.

```{python}
#| results: markup
potential_variables = cdh.list_variables(
    patterns=[
        r"Average household size --!!Total:$",
        r"Median household income in the past 12 months \(in 202\d inflation-adjusted dollars\)$"
    ],
    logic=any
)

for variable in potential_variables:
    print(variable['name'], "\n", variable['concept'])

```

Based on this output we can see that we're interested in `B25010_001E` and `B19013_001E`--we can set these variables explicitly.

```{python}
#| results: markup
cdh.set_variables(
    names=["B25010_001E", "B19013_001E"]
)
```

## Step 4. Explore Geographic Summary Levels
Next we need to select the geographic level at which we want to get our estimates. Let's assume we're interested in block groups and filter accordingly. That should give us a small number of options, so we'll output and view the complete dictionaries.

```{python}
#| results: markup
potential_geos = cdh.list_geos(
    patterns="block group",
    to_dicts=True
)

pprint(potential_geos)
```

A couple of things to note here. First, we can see in the first dictionary that it is the one we're interested in (summary level 150). Second, the dictionary includes `'requires': ['state', 'county', 'tract']` which indicates that in order to get estimates for a set of block groups from the API, the query must specify the parent state, county, and tract. In other to get block groups from multiple tracts, we will need multiple API calls--one for every tract, each of which also has to provide a specific state and county.

## Step 5. Get Data
Notably, there are nearly 84,000 tracts in continental U.S.! To keep this example a little more bite-sized, we'll focus on just three counties in Colorado and the entire state of Wyoming (mostly to illustrate the flexibility we have in providing nesting information in `within`). Now, that's still a lot of tracts, but `CenDatHelper` does the hard work of figuring out all of the state-county-tract combinations and building/issuing the necessary API calls. First we set our summary level of interest, then we get the data. Let's see how that works.

```{python}
%%time
#| results: markup

cdh.set_geos("150")

response = cdh.get_data(
    max_workers=50, 
    within=[
        {"state": "08", "county": ["123", "013"]}, 
        {"state": "08", "county": "069", "tract": ["001307", "001810","001308"]},
        {"state": "56"}
    ]
)
```

We can see that in the end 648 API calls were needed to satisfy our request, but due to the wonders of thread pooling, it didn't take very long at all. Note that this example illustrates the flexibility of `within` pretty well--we can specify parent geographies as a list of dictionaries at different levels of the geographic hierarchy above the summary level of interest. Since we're pulling block groups, we can provide state, state and county, or state, county and tract. Within each dictionary, the last item's values may be provided as a list. We see that above for counties in the first dictionary and tracts in the second.

## Step 6. Convert
Our final step is to convert our output to a more friendly format.

```{python}
#| results: markup
df = pl.concat(
        response.to_polars(
            schema_overrides={
                "B25010_001E": pl.Float64, 
                "B19013_001E": pl.Float64,
            }
    )
)

print(df.head())
```

Here I've chosen to convert to polars DataFrames (the response is a list with entries for each product vintage, so `to_polars()` generates a list of DataFrames - `pl.concat()` stacks them). I've also forced my estimate variables to float type, as they come from the API as strings.

That's all for now! Development is ongoing, and I plan to add new features as they occur to me. Don't hesitate to reach out of there's anything you'd like to see (or if you spot any bugs).